{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19c7377",
   "metadata": {},
   "source": [
    "# Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72bf2f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "from bespokelabs import curator\n",
    "\n",
    "# Fix sys.path and import paths for notebook execution\n",
    "sys.path.append(\"/home/yigit/codebase/gsw-memory/src/gsw_memory/\")\n",
    "from gsw_memory.memory.models import GSWStructure\n",
    "from gsw_memory.memory.operator_utils import parse_gsw, GSWOperator\n",
    "from gsw_memory.prompts.operator_prompts import PromptType, FactualExtractionPrompts\n",
    "from openai.lib._parsing._completions import type_to_response_format_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25479f40",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b013707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://127.0.0.1:6380/v1\", api_key=\"token-abc123\")\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"Qwen/Qwen3-8B\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}],\n",
    "  temperature=0.6,\n",
    "    extra_body={\n",
    "                \"temperature\": 0.6, \n",
    "                \"top_p\": 0.95, \n",
    "                \"top_k\": 20, \n",
    "                \"min_p\": 0, \n",
    "                \"max_tokens\": 4096, \n",
    "                \"repetition_penalty\": 1.1,\n",
    "                \"presence_penalty\": 0.3,\n",
    "                \"frequency_penalty\": 0.3\n",
    "                            }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f31f5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-4e6fd883ca15494096f5aea88fec095e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\\n\\nLarge language models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text based on vast amounts of training data. By analyzing patterns in language, they can perform tasks like answering questions, creating content, translating languages, and even coding. These models rely on complex neural network architectures (e.g., transformers) and require extensive computational resources to train. While they excel at mimicking human communication, challenges remain—including addressing biases in training data and ensuring ethical use. LLMs have revolutionized fields like customer service, education, and creative writing by enabling more intuitive interactions with technology.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=\"\\nOkay, the user wants a short introduction to large language models. Let me start by defining what they are. They're AI systems trained on massive text data, right? So I should mention that.\\n\\nI need to highlight their key features: understanding and generating human-like text across various tasks. Maybe list some examples like answering questions or writing stories. Also, mention things like natural language processing and machine learning techniques as part of how they work.\\n\\nWait, the user might not know terms like transformer architecture. Should I explain that briefly? But since it's supposed to be short, maybe just refer to them as advanced neural networks without diving into specifics.\\n\\nAlso, important to note their applications—like chatbots, content creation, coding assistance. That shows their real-world use cases. Oh, and touch on challenges such as biases in training data and ethical concerns. That adds depth but keeps it concise.\\n\\nMake sure the tone is clear and accessible, avoiding jargon where possible. Keep each sentence straightforward. Check if there's anything else the user might need—maybe mention ongoing research or improvements? Probably not necessary for a short intro.\\n\\nLet me structure this: start with definition, then capabilities, how they're built (training on vast data), applications, and finally challenges. That flows logically.\\n\"), stop_reason=None, token_ids=None)], created=1767998729, model='Qwen/Qwen3-8B', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=382, prompt_tokens=18, total_tokens=400, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None, prompt_token_ids=None, kv_transfer_params=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e96737",
   "metadata": {},
   "source": [
    "# Load Musique Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68d116ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_musique = json.load(open(\"/home/yigit/codebase/gsw-memory/playground_data/musique.json\"))\n",
    "# Load Musique Train Data jsonl to json format\n",
    "train_musique = [json.loads(line) for line in open(\"/home/yigit/codebase/gsw-memory/playground_data/musique_full_v1.0_train.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "873daa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_musique_corpus = {}\n",
    "for data in train_musique:\n",
    "    paragraphs = data[\"paragraphs\"]\n",
    "    for doc_idx, paragraph in enumerate(paragraphs):\n",
    "        train_musique_corpus[str(data[\"id\"]) + \"_\" + str(paragraph[\"idx\"])] = {\n",
    "            \"global_id\": f\"{data['id']}_{paragraph['idx']}\",\n",
    "            \"title\": paragraph[\"title\"],\n",
    "            \"text\": paragraph[\"title\"] + \"\\n\" + paragraph[\"paragraph_text\"],\n",
    "            \"id\": data[\"id\"],\n",
    "            \"idx\": paragraph[\"idx\"]\n",
    "        }\n",
    "        \n",
    "        \n",
    "test_musique_corpus = {}\n",
    "for data in test_musique:\n",
    "    paragraphs = data[\"paragraphs\"]\n",
    "    for doc_idx, paragraph in enumerate(paragraphs):\n",
    "        test_musique_corpus[str(data[\"id\"]) + \"_\" + str(paragraph[\"idx\"])] = {\n",
    "            \"global_id\": f\"{data['id']}_{paragraph['idx']}\",\n",
    "            \"title\": paragraph[\"title\"],\n",
    "            \"text\": paragraph[\"title\"] + \"\\n\" + paragraph[\"paragraph_text\"],\n",
    "            \"id\": data[\"id\"],\n",
    "            \"idx\": paragraph[\"idx\"]\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41bf6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 1000 documents from train_musique_corpus\n",
    "train_musique_corpus_sample = random.sample(list(train_musique_corpus.values()), 100)\n",
    "test_musique_corpus_sample = list(test_musique_corpus.values())[:250]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f5822",
   "metadata": {},
   "source": [
    "# Process Documents with Qwen3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5788953",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsw_completions = []\n",
    "SYSTEM_PROMPT = FactualExtractionPrompts.SYSTEM_PROMPT\n",
    "USER_PROMPT_TEMPLATE = FactualExtractionPrompts.USER_PROMPT_TEMPLATE\n",
    "USER_PROMPT = USER_PROMPT_TEMPLATE.format(input_text=train_musique_corpus_sample[1], background_context=\"\")\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:6380/v1\", api_key=\"token-abc123\")\n",
    "gsw_completions.append(client.chat.completions.parse(\n",
    "    model=\"Qwen/Qwen3-8B\",\n",
    "    messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": USER_PROMPT}],\n",
    "    temperature=0.6,\n",
    "    response_format=GSWStructure,\n",
    "    extra_body={\n",
    "                    # \"temperature\": 0.1,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"top_k\": 20,\n",
    "                    \"min_p\": 0.0,\n",
    "                    \"max_tokens\": 4096 * 3, \n",
    "                    \"repetition_penalty\": 1.1,\n",
    "                    \"presence_penalty\": 0.3,\n",
    "                    \"frequency_penalty\": 0.3\n",
    "                            }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb4cb8",
   "metadata": {},
   "source": [
    "# Test On Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2c18b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/09/26 16:58:32] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Getting rate limits for model:                 <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">litellm_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py#214\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         hosted_vllm/Qwen/Qwen3-8B                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/09/26 16:58:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Getting rate limits for model:                 \u001b]8;id=825569;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py\u001b\\\u001b[2mlitellm_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=704897;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py#214\u001b\\\u001b[2m214\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         hosted_vllm/Qwen/Qwen3-8B                      \u001b[2m                                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 6 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='choices', input_value=Message(content='\\n\\nHell...urther conversation.\\n'), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...rther conversation.\\n')), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/09/26 16:58:35] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> LiteLLM does not support cost estimation for   <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">litellm_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py#197\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         model: This model isn't mapped yet.            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #800080; text-decoration-color: #800080\">Qwen</span>/Qwen3-8B,                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #808000; text-decoration-color: #808000\">custom_llm_provider</span>=<span style=\"color: #800080; text-decoration-color: #800080\">hosted_vllm</span>. Add it here - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/BerriAI/litellm/blob/main/m</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">odel_prices_and_context_window.json.</span>           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/09/26 16:58:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m LiteLLM does not support cost estimation for   \u001b]8;id=120346;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py\u001b\\\u001b[2mlitellm_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=218469;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py#197\u001b\\\u001b[2m197\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         model: This model isn't mapped yet.            \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[33mmodel\u001b[0m=\u001b[35mQwen\u001b[0m/Qwen3-8B,                           \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[33mcustom_llm_provider\u001b[0m=\u001b[35mhosted_vllm\u001b[0m. Add it here - \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://github.com/BerriAI/litellm/blob/main/m\u001b[0m \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94model_prices_and_context_window.json.\u001b[0m           \u001b[2m                                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Test call headers: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'llm_provider-date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Sat,</span> <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">litellm_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py#200\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">10 Jan 2026 00:58:32 GMT'</span>,                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'llm_provider-server'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'uvicorn'</span>,              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'llm_provider-content-length'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1136'</span>,         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'llm_provider-content-type'</span>:                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'application/json'</span><span style=\"font-weight: bold\">}</span>                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Test call headers: \u001b[1m{\u001b[0m\u001b[32m'llm_provider-date'\u001b[0m: \u001b[32m'Sat,\u001b[0m \u001b]8;id=486618;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py\u001b\\\u001b[2mlitellm_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=124336;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/litellm_online_request_processor.py#200\u001b\\\u001b[2m200\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m10 Jan 2026 00:58:32 GMT'\u001b[0m,                     \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'llm_provider-server'\u001b[0m: \u001b[32m'uvicorn'\u001b[0m,              \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'llm_provider-content-length'\u001b[0m: \u001b[32m'1136'\u001b[0m,         \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'llm_provider-content-type'\u001b[0m:                   \u001b[2m                                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'application/json'\u001b[0m\u001b[1m}\u001b[0m                            \u001b[2m                                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"HOSTED_VLLM_API_KEY\"] = \"token-abc123\"\n",
    "gsw_model = GSWOperator(\n",
    "                model_name=\"hosted_vllm/Qwen/Qwen3-8B\",\n",
    "                backend_params = {\n",
    "                    \"base_url\": \"http://127.0.0.1:6380/v1\",\n",
    "                    \"request_timeout\": 600.0,  \n",
    "                    \"max_concurrent_requests\": 64,\n",
    "                    \"max_requests_per_minute\": 120,\n",
    "                    \"max_tokens_per_minute\": 200000,\n",
    "                    \"seconds_to_pause_on_rate_limit\": 5,\n",
    "                    \"require_all_responses\": False,\n",
    "                },\n",
    "                generation_params={\n",
    "                    \"temperature\": 0.6, \n",
    "                    \"top_p\": 0.95, \n",
    "                    \"top_k\": 20, \n",
    "                    \"min_p\": 0, \n",
    "                    \"max_tokens\": 4096 * 3, \n",
    "                    \"repetition_penalty\": 1.1,\n",
    "                    \"presence_penalty\": 0.3,\n",
    "                    \"frequency_penalty\": 0.3\n",
    "                            },\n",
    "                prompt_type=PromptType.FACTUAL,\n",
    "                backend=\"litellm\",\n",
    "                response_format=GSWStructure,  # Use constrained decoding\n",
    "                batch=False,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b49bf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6deeb094864aaf8a217d066c0ca228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/09/26 16:58:36] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Running LiteLLMOnlineRequestProcessor completions with   <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#133\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         model: hosted_vllm/Qwen/Qwen3-8B                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/09/26 16:58:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Running LiteLLMOnlineRequestProcessor completions with   \u001b]8;id=293416;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\u001b\\\u001b[2mbase_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=139429;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#133\u001b\\\u001b[2m133\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         model: hosted_vllm/Qwen/Qwen3-8B                         \u001b[2m                             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Preparing request <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">file</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> in                             <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#233\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">233</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/yigit/.cache/curator/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">7105ed9939df1c1b</span>              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Preparing request \u001b[1;35mfile\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m in                             \u001b]8;id=957534;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\u001b\\\u001b[2mbase_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=672215;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#233\u001b\\\u001b[2m233\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/yigit/.cache/curator/\u001b[0m\u001b[95m7105ed9939df1c1b\u001b[0m              \u001b[2m                             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Wrote <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> requests to                                      <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#315\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/yigit/.cache/curator/7105ed9939df1c1b/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">requests_0.j</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">sonl.</span>                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Wrote \u001b[1;36m1\u001b[0m requests to                                      \u001b]8;id=49070;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\u001b\\\u001b[2mbase_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=802896;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#315\u001b\\\u001b[2m315\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/yigit/.cache/curator/7105ed9939df1c1b/\u001b[0m\u001b[95mrequests_0.j\u001b[0m \u001b[2m                             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[95msonl.\u001b[0m                                                    \u001b[2m                             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Manually set max_requests_per_minute to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>       <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#191\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">191</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Manually set max_requests_per_minute to \u001b[1;36m120\u001b[0m       \u001b]8;id=180892;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\u001b\\\u001b[2mbase_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=594163;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#191\u001b\\\u001b[2m191\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Manually set max_tokens_per_minute to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200000</span>      <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#210\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">210</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Manually set max_tokens_per_minute to \u001b[1;36m200000\u001b[0m      \u001b]8;id=303210;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\u001b\\\u001b[2mbase_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=256341;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#210\u001b\\\u001b[2m210\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Manually set max_concurrent_requests to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>        <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#174\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Manually set max_concurrent_requests to \u001b[1;36m64\u001b[0m        \u001b]8;id=981522;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\u001b\\\u001b[2mbase_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=530027;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#174\u001b\\\u001b[2m174\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Manually set max_concurrent_requests to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>        <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#174\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Manually set max_concurrent_requests to \u001b[1;36m64\u001b[0m        \u001b]8;id=218623;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\u001b\\\u001b[2mbase_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=216129;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#174\u001b\\\u001b[2m174\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10807313980d475d9b4706a56dd0055e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic \n",
       "serializer warnings:\n",
       "  PydanticSerializationUnexpectedValue(Expected 6 fields but got 5: Expected `Message` - serialized value may not \n",
       "be as expected [field_name='choices', input_value=Message(content='', role=...es provided earlier.\\n'), \n",
       "input_type=Message])\n",
       "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected \n",
       "[field_name='choices', input_value=Choices(finish_reason='st...s provided earlier.\\n')), input_type=Choices])\n",
       "  return self.__pydantic_serializer__.to_python(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic \n",
       "serializer warnings:\n",
       "  PydanticSerializationUnexpectedValue(Expected 6 fields but got 5: Expected `Message` - serialized value may not \n",
       "be as expected [field_name='choices', input_value=Message(content='', role=...es provided earlier.\\n'), \n",
       "input_type=Message])\n",
       "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected \n",
       "[field_name='choices', input_value=Choices(finish_reason='st...s provided earlier.\\n')), input_type=Choices])\n",
       "  return self.__pydantic_serializer__.to_python(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">•</span> Time Elapsed <span style=\"color: #808000; text-decoration-color: #808000\">0:01:01</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">•</span> Time Remaining <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[1;37m•\u001b[0m Time Elapsed \u001b[33m0:01:01\u001b[0m \u001b[1;37m•\u001b[0m Time Remaining \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Curator Viewer:</span> <span style=\"color: #808000; text-decoration-color: #808000\">Disabled</span>                                                                                  \n",
       "Set <span style=\"color: #808000; text-decoration-color: #808000\">CURATOR_VIEWER=</span><span style=\"color: #008080; text-decoration-color: #008080\">1</span> to view your data live at <span style=\"color: #000080; text-decoration-color: #000080\">https://curator.bespokelabs.ai</span>                             \n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Requests:</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Total:</span> <span style=\"color: #000080; text-decoration-color: #000080\">1</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Cached:</span> <span style=\"color: #008000; text-decoration-color: #008000\">0✓</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Success:</span> <span style=\"color: #008000; text-decoration-color: #008000\">1✓</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Failed:</span> <span style=\"color: #800000; text-decoration-color: #800000\">0✗</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">In Progress:</span> <span style=\"color: #808000; text-decoration-color: #808000\">0⋯</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Req/min:</span> <span style=\"color: #000080; text-decoration-color: #000080\">1.0</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Res/min:</span> <span style=\"color: #000080; text-decoration-color: #000080\">1.0</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Tokens:</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Avg Input:</span> <span style=\"color: #000080; text-decoration-color: #000080\">11443</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Input TPM:</span> <span style=\"color: #000080; text-decoration-color: #000080\">11142</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Avg Output:</span> <span style=\"color: #000080; text-decoration-color: #000080\">2013</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Output TPM:</span> <span style=\"color: #000080; text-decoration-color: #000080\">1960</span>                         \n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Cost:</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Current:</span> <span style=\"color: #800080; text-decoration-color: #800080\">$0.000</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Est. Total:</span> <span style=\"color: #800080; text-decoration-color: #800080\">$0.000</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">($0.000 remaining)</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Rate:</span> <span style=\"color: #800080; text-decoration-color: #800080\">$0.000/min</span>                          \n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Rate Limits:</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">RPM:</span> <span style=\"color: #000080; text-decoration-color: #000080\">120</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">TPM:</span> <span style=\"color: #000080; text-decoration-color: #000080\">200000</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">TPM Strategy:</span> <span style=\"color: #000080; text-decoration-color: #000080\">combined token limit</span>                                  \n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Model:</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name:</span> <span style=\"color: #000080; text-decoration-color: #000080\">hosted_vllm/Qwen/Qwen3-8B</span>                                                                    \n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Model Pricing:</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Per 1M tokens:</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Input:</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">N/A</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">•</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Output:</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">N/A</span>                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;37mCurator Viewer:\u001b[0m \u001b[33mDisabled\u001b[0m                                                                                  \n",
       "Set \u001b[33mCURATOR_VIEWER=\u001b[0m\u001b[36m1\u001b[0m to view your data live at \u001b[34mhttps://curator.bespokelabs.ai\u001b[0m                             \n",
       "\u001b[1;37mRequests:\u001b[0m \u001b[37mTotal:\u001b[0m \u001b[34m1\u001b[0m \u001b[37m•\u001b[0m \u001b[37mCached:\u001b[0m \u001b[32m0✓\u001b[0m \u001b[37m•\u001b[0m \u001b[37mSuccess:\u001b[0m \u001b[32m1✓\u001b[0m \u001b[37m•\u001b[0m \u001b[37mFailed:\u001b[0m \u001b[31m0✗\u001b[0m \u001b[37m•\u001b[0m \u001b[37mIn Progress:\u001b[0m \u001b[33m0⋯\u001b[0m \u001b[37m•\u001b[0m \u001b[37mReq/min:\u001b[0m \u001b[34m1.0\u001b[0m \u001b[37m•\u001b[0m \u001b[37mRes/min:\u001b[0m \u001b[34m1.0\u001b[0m\n",
       "\u001b[1;37mTokens:\u001b[0m \u001b[37mAvg Input:\u001b[0m \u001b[34m11443\u001b[0m \u001b[37m•\u001b[0m \u001b[37mInput TPM:\u001b[0m \u001b[34m11142\u001b[0m \u001b[37m•\u001b[0m \u001b[37mAvg Output:\u001b[0m \u001b[34m2013\u001b[0m \u001b[37m•\u001b[0m \u001b[37mOutput TPM:\u001b[0m \u001b[34m1960\u001b[0m                         \n",
       "\u001b[1;37mCost:\u001b[0m \u001b[37mCurrent:\u001b[0m \u001b[35m$0.000\u001b[0m \u001b[37m•\u001b[0m \u001b[37mEst. Total:\u001b[0m \u001b[35m$0.000\u001b[0m \u001b[2m($0.000 remaining)\u001b[0m \u001b[37m•\u001b[0m \u001b[37mRate:\u001b[0m \u001b[35m$0.000/min\u001b[0m                          \n",
       "\u001b[1;37mRate Limits:\u001b[0m \u001b[37mRPM:\u001b[0m \u001b[34m120\u001b[0m \u001b[37m•\u001b[0m \u001b[37mTPM:\u001b[0m \u001b[34m200000\u001b[0m \u001b[37m•\u001b[0m \u001b[37mTPM Strategy:\u001b[0m \u001b[34mcombined token limit\u001b[0m                                  \n",
       "\u001b[1;37mModel:\u001b[0m \u001b[37mName:\u001b[0m \u001b[34mhosted_vllm/Qwen/Qwen3-8B\u001b[0m                                                                    \n",
       "\u001b[1;37mModel Pricing:\u001b[0m \u001b[37mPer 1M tokens:\u001b[0m \u001b[37mInput:\u001b[0m \u001b[2mN/A\u001b[0m \u001b[37m•\u001b[0m \u001b[37mOutput:\u001b[0m \u001b[2mN/A\u001b[0m                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                 Final Curator Statistics                 </span>\n",
       "╭────────────────────────────┬───────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Section/Metric             </span>│<span style=\"font-weight: bold\"> Value                     </span>│\n",
       "├────────────────────────────┼───────────────────────────┤\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Model                      </span>│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Name                       </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">hosted_vllm/Qwen/Qwen3-8B</span><span style=\"color: #808000; text-decoration-color: #808000\"> </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rate Limit (RPM)           </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">120</span><span style=\"color: #808000; text-decoration-color: #808000\">                       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rate Limit (TPM)           </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #000080; text-decoration-color: #000080\">200000</span><span style=\"color: #808000; text-decoration-color: #808000\">                    </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Requests                   </span>│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Processed            </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 1                         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Successful                 </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">                         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Failed                     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #800000; text-decoration-color: #800000\">0</span><span style=\"color: #808000; text-decoration-color: #808000\">                         </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Tokens                     </span>│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Tokens Used          </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 13,456                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Input Tokens         </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 11,443                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Output Tokens        </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2,013                     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Tokens per Request </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 13456                     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Input Tokens       </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 11443                     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Output Tokens      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2013                      </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Costs                      </span>│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Cost                 </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #800000; text-decoration-color: #800000\">$0.000</span><span style=\"color: #808000; text-decoration-color: #808000\">                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Cost per Request   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #800000; text-decoration-color: #800000\">$0.000</span><span style=\"color: #808000; text-decoration-color: #808000\">                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Input Cost per 1M Tokens   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">N/A</span><span style=\"color: #808000; text-decoration-color: #808000\">                       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Output Cost per 1M Tokens  </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">N/A</span><span style=\"color: #808000; text-decoration-color: #808000\">                       </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Performance                </span>│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Time                 </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 61.63s                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Time per Request   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 61.63s                    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Requests per Minute        </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 1.0                       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Responses per Minute       </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 1.0                       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Max Concurrent Requests    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 1                         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Input Tokens per Minute    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 11140.0                   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Output Tokens per Minute   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 1959.7                    </span>│\n",
       "╰────────────────────────────┴───────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                 Final Curator Statistics                 \u001b[0m\n",
       "╭────────────────────────────┬───────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mSection/Metric            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "├────────────────────────────┼───────────────────────────┤\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mModel                     \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35m                         \u001b[0m\u001b[1;35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mName                      \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[34mhosted_vllm/Qwen/Qwen3-8B\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRate Limit (RPM)          \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[34m120\u001b[0m\u001b[33m                      \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRate Limit (TPM)          \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[34m200000\u001b[0m\u001b[33m                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mRequests                  \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35m                         \u001b[0m\u001b[1;35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Processed           \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1                        \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mSuccessful                \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[32m1\u001b[0m\u001b[33m                        \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFailed                    \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[31m0\u001b[0m\u001b[33m                        \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mTokens                    \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35m                         \u001b[0m\u001b[1;35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Tokens Used         \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m13,456                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Input Tokens        \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m11,443                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Output Tokens       \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m2,013                    \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Tokens per Request\u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m13456                    \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Input Tokens      \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m11443                    \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Output Tokens     \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m2013                     \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mCosts                     \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35m                         \u001b[0m\u001b[1;35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Cost                \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[31m$0.000\u001b[0m\u001b[33m                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Cost per Request  \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[31m$0.000\u001b[0m\u001b[33m                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mInput Cost per 1M Tokens  \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[2;33mN/A\u001b[0m\u001b[33m                      \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mOutput Cost per 1M Tokens \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[2;33mN/A\u001b[0m\u001b[33m                      \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mPerformance               \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;35m \u001b[0m\u001b[1;35m                         \u001b[0m\u001b[1;35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Time                \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m61.63s                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Time per Request  \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m61.63s                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRequests per Minute       \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1.0                      \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mResponses per Minute      \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1.0                      \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mMax Concurrent Requests   \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1                        \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mInput Tokens per Minute   \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m11140.0                  \u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mOutput Tokens per Minute  \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1959.7                   \u001b[0m\u001b[33m \u001b[0m│\n",
       "╰────────────────────────────┴───────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/09/26 16:59:37] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Processing complete. Results saved to             <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#459\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">459</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/yigit/.cache/curator/7105ed9939df1c1b/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">respo</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                    </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">nses_0.jsonl</span>                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/09/26 16:59:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing complete. Results saved to             \u001b]8;id=155279;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\u001b\\\u001b[2mbase_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=302415;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#459\u001b\\\u001b[2m459\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/yigit/.cache/curator/7105ed9939df1c1b/\u001b[0m\u001b[95mrespo\u001b[0m \u001b[2m                                    \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[95mnses_0.jsonl\u001b[0m                                      \u001b[2m                                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Status tracker: Tasks - Started: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, In Progress:  <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_online_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#460\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">460</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, Succeeded: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, Failed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, Already Completed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                    </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Errors - API: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, Rate Limit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, Other: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, Total:  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                    </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Status tracker: Tasks - Started: \u001b[1;36m1\u001b[0m, In Progress:  \u001b]8;id=431034;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py\u001b\\\u001b[2mbase_online_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=815962;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/online/base_online_request_processor.py#460\u001b\\\u001b[2m460\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0\u001b[0m, Succeeded: \u001b[1;36m1\u001b[0m, Failed: \u001b[1;36m0\u001b[0m, Already Completed: \u001b[1;36m0\u001b[0m  \u001b[2m                                    \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         Errors - API: \u001b[1;36m0\u001b[0m, Rate Limit: \u001b[1;36m0\u001b[0m, Other: \u001b[1;36m0\u001b[0m, Total:  \u001b[2m                                    \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0\u001b[0m                                                 \u001b[2m                                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Read <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> responses.                                        <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#444\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">444</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Read \u001b[1;36m1\u001b[0m responses.                                        \u001b]8;id=764613;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\u001b\\\u001b[2mbase_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=970540;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#444\u001b\\\u001b[2m444\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Finalizing writer                                        <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#453\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">453</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Finalizing writer                                        \u001b]8;id=364288;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\u001b\\\u001b[2mbase_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=57248;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#453\u001b\\\u001b[2m453\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/09/26 16:59:38] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Creating a file with all failed requests                 <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#462\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">462</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/09/26 16:59:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Creating a file with all failed requests                 \u001b]8;id=244349;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\u001b\\\u001b[2mbase_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=877433;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#462\u001b\\\u001b[2m462\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Created file with failed requests at                     <a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_request_processor.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#490\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">490</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/yigit/.cache/curator/7105ed9939df1c1b/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">failed_reque</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">sts.jsonl</span>                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Created file with failed requests at                     \u001b]8;id=801206;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py\u001b\\\u001b[2mbase_request_processor.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=680546;file:///home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/bespokelabs/curator/request_processor/base_request_processor.py#490\u001b\\\u001b[2m490\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/home/yigit/.cache/curator/7105ed9939df1c1b/\u001b[0m\u001b[95mfailed_reque\u001b[0m \u001b[2m                             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[95msts.jsonl\u001b[0m                                                \u001b[2m                             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os.environ[\"CURATOR_DISABLE_CACHE\"] = \"1\"\n",
    "gsw_response = gsw_model([train_musique_corpus_sample[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c49391d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents_data = {}\n",
    "for response in gsw_response.dataset: #TODO: Check if this is correct\n",
    "    try:\n",
    "        if response[\"gsw\"]:\n",
    "            gsw_dict = response[\"gsw\"]\n",
    "            gsw = GSWStructure(**gsw_dict)\n",
    "        else:\n",
    "            gsw = parse_gsw(response[\"graph\"])\n",
    "        doc_idx = response[\"doc_idx\"]\n",
    "        global_id = response[\"global_id\"]\n",
    "        all_documents_data[global_id] = gsw\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error parsing GSW for chunk {response.get('global_id', 'unknown')}: {e}\"\n",
    "        )\n",
    "        continue\n",
    "pred_gsws = list(all_documents_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df902a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load first 100 documents from musique processed docs\n",
    "import glob\n",
    "musique_network_dir = '/mnt/SSD1/shreyas/SM_GSW/musique/networks_4_1_mini'\n",
    "import re\n",
    "\n",
    "def sort_natural_key(s):\n",
    "    # Extract the numeric part from the path string\n",
    "    match = re.search(r'doc_(\\d+)', s)\n",
    "    return int(match.group(1)) if match else s\n",
    "\n",
    "musique_docs = sorted(\n",
    "    glob.glob(f\"{musique_network_dir}/doc_*\"),\n",
    "    key=sort_natural_key\n",
    ")\n",
    "musique_docs_0_99 = musique_docs[:100]\n",
    "\n",
    "# go in each dir in musique_docs_0_99 and load each json file\n",
    "import os\n",
    "\n",
    "golden_gsws = []\n",
    "for doc_dir in musique_docs_0_99:\n",
    "    if os.path.isdir(doc_dir):\n",
    "        json_files = sorted(glob.glob(os.path.join(doc_dir, \"*.json\")))\n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                with open(json_file, 'r') as f:\n",
    "                    doc_data = json.load(f)\n",
    "                    doc_data = GSWStructure(**doc_data)\n",
    "                    golden_gsws.append(doc_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {json_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: {doc_dir} is not a directory, skipping.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7de3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_AS_A_JUDGE_PROMPT =\"\"\"SYSTEM:\n",
    "You are a strict evaluator (LLM-as-a-judge) for GSW factual extraction graphs used in multi-hop QA.\n",
    "You will compare a PREDICTED GSW against a GOLD GSW for the SAME input text.\n",
    "\n",
    "Your job:\n",
    "1) Determine how well the predicted GSW matches the gold GSW in factual coverage and structural compliance.\n",
    "2) Identify missing facts, hallucinated facts, malformed entities, malformed relations, and question-format violations.\n",
    "3) Output ONLY valid JSON in the schema given below. No extra commentary.\n",
    "\n",
    "Definitions:\n",
    "- A \"fact\" is a (subject entity, verb phrase, object entity) triple expressed by a verb_phrase_node whose questions imply that relation.\n",
    "- \"Coverage\" means the predicted GSW contains an equivalent fact to one in the gold GSW.\n",
    "- \"Equivalent\" means same underlying meaning, allowing:\n",
    "  - minor wording differences in the verb phrase/questions,\n",
    "  - re-ordering,\n",
    "  - synonyms that do not change meaning (e.g., \"joined\" vs \"enrolled\" if clearly same in context),\n",
    "  - but NOT allowing changed entities, times, locations, roles, or added details not in text.\n",
    "- IDs may differ across graphs; match by entity name + type/role + context.\n",
    "\n",
    "Hard constraints from the extraction spec (judge these strictly):\n",
    "A) No fabrication: predicted facts must be entailed by the input text.\n",
    "B) Atomic entities: do not bundle multiple separable entities into one.\n",
    "C) Abbreviation/alias: if text has \"Full Name (ABBR)\", predicted should create:\n",
    "   - entity \"Full Name (ABBR)\" and entity \"ABBR\" (alias) connected via \"also known as\".\n",
    "   - Do NOT expand abbreviations that were not expanded in text.\n",
    "D) Two questions per verb phrase node: exactly 2 questions per verb_phrase_node.\n",
    "E) Each question must have exactly one unknown (answer) and no pronouns (\"he\", \"it\", \"they\", etc.).\n",
    "F) Questions must contain complete content (no dropping \"that ...\" clauses when required).\n",
    "G) Temporal connectivity: when gold connects a date/time via a phrase like \"on/in/during\", predicted should also connect that temporal info somewhere relevant.\n",
    "H) Answers must be entity IDs only and must exist in entity_nodes.\n",
    "I) Do not merge multiple subjects/objects into one question unless gold does (and the rule permits it).\n",
    "\n",
    "Evaluation procedure:\n",
    "1) Parse both GSWs into:\n",
    "   - entity inventory: (name, role(s), states)\n",
    "   - fact inventory: for each verb_phrase_node, infer intended (S, P, O) from the questions:\n",
    "     * If one question is \"Who <P> <O> ...?\" and the answer is X => (X, P, O)\n",
    "     * If one question is \"What/Which <O> did <S> <P> ...?\" and the answer is Y => (S, P, Y)\n",
    "   If inference is ambiguous, record it as \"unscorable_relation\" and penalize format/clarity, not factual mismatch.\n",
    "2) Align entities between gold and pred by best match on:\n",
    "   - exact name match > normalized match (case/diacritics) > alias match\n",
    "   - role compatibility (person vs org vs date etc.)\n",
    "3) Align facts:\n",
    "   - A gold fact is \"covered\" if an equivalent pred fact exists.\n",
    "   - A pred fact is \"hallucinated\" if it is not entailed by input text OR has no corresponding gold fact meaningfully.\n",
    "4) Score:\n",
    "   - Coverage (0–1): covered_gold_facts / total_gold_facts\n",
    "   - Precision (0–1): correct_pred_facts / total_pred_facts (exclude unscorable if clearly malformed)\n",
    "   - Format compliance (0–1): start from 1 and subtract for each violation category (see below).\n",
    "   - Overall (0–100): weighted:\n",
    "       overall = 45*coverage + 35*precision + 20*format_compliance\n",
    "   Provide also pass/fail for \"usable_for_QA\" with threshold overall>=80 and no critical violations.\n",
    "Critical violations (any => usable_for_QA=false even if score high):\n",
    "   - fabrication (A)\n",
    "   - pronouns in questions (E)\n",
    "   - answers not IDs / missing IDs (H)\n",
    "   - not exactly two questions per verb phrase node (D)\n",
    "\n",
    "Format compliance penalties (examples):\n",
    "- two_questions_violation: -0.10 each verb phrase node violating\n",
    "- pronoun_violation: -0.15 each occurrence\n",
    "- missing_that_content: -0.10 each occurrence\n",
    "- entity_bundling: -0.10 each bundled entity\n",
    "- alias_rule_violation: -0.10 each missed/incorrect alias case\n",
    "- temporal_disconnection: -0.05 each missing required time link\n",
    "- bad_answer_ids: critical (set compliance to 0 for that node and mark critical)\n",
    "\n",
    "Output JSON schema (MUST follow exactly):\n",
    "{{\n",
    "  \"overall_score\": 0-100 number,\n",
    "  \"usable_for_QA\": boolean,\n",
    "  \"subscores\": {{\n",
    "    \"coverage\": 0-1 number,\n",
    "    \"precision\": 0-1 number,\n",
    "    \"format_compliance\": 0-1 number\n",
    "  }},\n",
    "  \"counts\": {{\n",
    "    \"gold_entities\": int,\n",
    "    \"pred_entities\": int,\n",
    "    \"gold_facts\": int,\n",
    "    \"pred_facts\": int,\n",
    "    \"covered_gold_facts\": int,\n",
    "    \"hallucinated_pred_facts\": int,\n",
    "    \"unscorable_pred_relations\": int\n",
    "  }},\n",
    "  \"critical_violations\": [\n",
    "    {{\n",
    "      \"type\": \"fabrication|pronouns|bad_answer_ids|two_questions_violation\",\n",
    "      \"message\": \"short explanation\",\n",
    "      \"location\": \"entity_id or verb_phrase_id or question_id\"\n",
    "    }}\n",
    "  ],\n",
    "  \"entity_alignment\": [\n",
    "    {{\n",
    "      \"gold_entity_id\": \"e#\",\n",
    "      \"pred_entity_id\": \"e# or null\",\n",
    "      \"match_type\": \"exact|normalized|alias|none\",\n",
    "      \"notes\": \"brief\"\n",
    "    }}\n",
    "  ],\n",
    "  \"missing_entities\": [\n",
    "    {{ \"gold_entity_id\": \"e#\", \"gold_name\": \"...\", \"reason\": \"missing|role_mismatch|bundled\" }}\n",
    "  ],\n",
    "  \"extra_entities\": [\n",
    "    {{ \"pred_entity_id\": \"e#\", \"pred_name\": \"...\", \"reason\": \"hallucinated|unnecessary|bundled\" }}\n",
    "  ],\n",
    "  \"fact_comparison\": {{\n",
    "    \"missing_facts\": [\n",
    "      {{\n",
    "        \"gold_fact\": {{ \"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}},\n",
    "        \"gold_verb_phrase_id\": \"v#\",\n",
    "        \"reason\": \"missing|temporal_missing|wrong_object|wrong_subject\"\n",
    "      }}\n",
    "    ],\n",
    "    \"covered_facts\": [\n",
    "      {{\n",
    "        \"gold_fact\": {{ \"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}},\n",
    "        \"pred_fact\": {{ \"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}},\n",
    "        \"gold_verb_phrase_id\": \"v#\",\n",
    "        \"pred_verb_phrase_id\": \"v#\",\n",
    "        \"match_notes\": \"brief\"\n",
    "      }}\n",
    "    ],\n",
    "    \"hallucinated_facts\": [\n",
    "      {{\n",
    "        \"pred_fact\": {{ \"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}},\n",
    "        \"pred_verb_phrase_id\": \"v#\",\n",
    "        \"reason\": \"not_in_gold|not_entailed_by_text|over_specific\"\n",
    "      }}\n",
    "    ]\n",
    "  }},\n",
    "  \"format_issues\": [\n",
    "    {{\n",
    "      \"type\": \"two_questions_violation|pronoun_violation|missing_that_content|alias_rule_violation|entity_bundling|temporal_disconnection|question_unknown_count|other\",\n",
    "      \"message\": \"short explanation\",\n",
    "      \"location\": \"v# or q# or e#\"\n",
    "    }}\n",
    "  ],\n",
    "  \"improvement_suggestions\": [\n",
    "    \"bullet-like string suggestions, concrete and minimal\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "USER (template you will receive):\n",
    "<input_text>\n",
    "{input_text}\n",
    "</input_text>\n",
    "\n",
    "<gold_gsw_json>\n",
    "{gold_gsw_json}\n",
    "</gold_gsw_json>\n",
    "\n",
    "<pred_gsw_json>\n",
    "{pred_gsw_json}\n",
    "</pred_gsw_json>\n",
    "\n",
    "Now perform the evaluation and output ONLY the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Subscores(BaseModel):\n",
    "    \"\"\"Subscores for coverage, precision, and format compliance.\"\"\"\n",
    "    coverage: float = Field(description=\"Coverage score (0-1)\")\n",
    "    precision: float = Field(description=\"Precision score (0-1)\")\n",
    "    format_compliance: float = Field(description=\"Format compliance score (0-1)\")\n",
    "\n",
    "\n",
    "class Counts(BaseModel):\n",
    "    \"\"\"Counts of gold and predicted entities, facts, and violations.\"\"\"\n",
    "    gold_entities: int = Field(description=\"Number of gold entities\")\n",
    "    pred_entities: int = Field(description=\"Number of predicted entities\")\n",
    "    gold_facts: int = Field(description=\"Number of gold facts\")\n",
    "    pred_facts: int = Field(description=\"Number of predicted facts\")\n",
    "    covered_gold_facts: int = Field(description=\"Number of covered gold facts\")\n",
    "    hallucinated_pred_facts: int = Field(description=\"Number of hallucinated predicted facts\")\n",
    "    unscorable_pred_relations: int = Field(description=\"Number of unscorable predicted relations\")\n",
    "\n",
    "\n",
    "class CriticalViolation(BaseModel):\n",
    "    \"\"\"A critical violation in the GSW.\"\"\"\n",
    "    type: str = Field(description=\"Type of violation\")\n",
    "    message: str = Field(description=\"Short explanation\")\n",
    "    location: str = Field(description=\"Entity ID, verb phrase ID, or question ID\")\n",
    "\n",
    "\n",
    "class EntityAlignment(BaseModel):\n",
    "    \"\"\"Alignment between gold and predicted entities.\"\"\"\n",
    "    gold_entity_id: str = Field(description=\"Gold entity ID\")\n",
    "    pred_entity_id: Optional[str] = Field(description=\"Predicted entity ID or null\", default=None)\n",
    "    match_type: str = Field(description=\"Match type: exact, normalized, alias, or none\")\n",
    "    notes: str = Field(description=\"Brief notes\")\n",
    "\n",
    "\n",
    "class MissingEntity(BaseModel):\n",
    "    \"\"\"An entity missing from the predicted GSW.\"\"\"\n",
    "    gold_entity_id: str = Field(description=\"Gold entity ID\")\n",
    "    gold_name: str = Field(description=\"Gold entity name\")\n",
    "    reason: str = Field(description=\"Reason: missing, role_mismatch, or bundled\")\n",
    "\n",
    "\n",
    "class ExtraEntity(BaseModel):\n",
    "    \"\"\"An extra entity in the predicted GSW.\"\"\"\n",
    "    pred_entity_id: str = Field(description=\"Predicted entity ID\")\n",
    "    pred_name: str = Field(description=\"Predicted entity name\")\n",
    "    reason: str = Field(description=\"Reason: hallucinated, unnecessary, or bundled\")\n",
    "\n",
    "\n",
    "class FactDetail(BaseModel):\n",
    "    \"\"\"Details of a fact (subject, predicate, object triple).\"\"\"\n",
    "    subject: str = Field(description=\"Subject entity\")\n",
    "    predicate: str = Field(description=\"Predicate/verb phrase\")\n",
    "    object: str = Field(description=\"Object entity\")\n",
    "\n",
    "\n",
    "class MissingFact(BaseModel):\n",
    "    \"\"\"A fact missing from the predicted GSW.\"\"\"\n",
    "    gold_fact: FactDetail = Field(description=\"Gold fact details\")\n",
    "    gold_verb_phrase_id: str = Field(description=\"Gold verb phrase ID\")\n",
    "    reason: str = Field(description=\"Reason: missing, temporal_missing, wrong_object, or wrong_subject\")\n",
    "\n",
    "\n",
    "class CoveredFact(BaseModel):\n",
    "    \"\"\"A fact covered by both gold and predicted GSW.\"\"\"\n",
    "    gold_fact: FactDetail = Field(description=\"Gold fact details\")\n",
    "    pred_fact: FactDetail = Field(description=\"Predicted fact details\")\n",
    "    gold_verb_phrase_id: str = Field(description=\"Gold verb phrase ID\")\n",
    "    pred_verb_phrase_id: str = Field(description=\"Predicted verb phrase ID\")\n",
    "    match_notes: str = Field(description=\"Brief match notes\")\n",
    "\n",
    "\n",
    "class HallucinatedFact(BaseModel):\n",
    "    \"\"\"A hallucinated fact in the predicted GSW.\"\"\"\n",
    "    pred_fact: FactDetail = Field(description=\"Predicted fact details\")\n",
    "    pred_verb_phrase_id: str = Field(description=\"Predicted verb phrase ID\")\n",
    "    reason: str = Field(description=\"Reason: not_in_gold, not_entailed_by_text, or over_specific\")\n",
    "\n",
    "\n",
    "class FactComparison(BaseModel):\n",
    "    \"\"\"Comparison of facts between gold and predicted GSW.\"\"\"\n",
    "    missing_facts: List[MissingFact] = Field(description=\"Facts missing from predicted GSW\", default_factory=list)\n",
    "    covered_facts: List[CoveredFact] = Field(description=\"Facts covered by both GSWs\", default_factory=list)\n",
    "    hallucinated_facts: List[HallucinatedFact] = Field(description=\"Hallucinated facts in predicted GSW\", default_factory=list)\n",
    "\n",
    "\n",
    "class FormatIssue(BaseModel):\n",
    "    \"\"\"A format issue in the predicted GSW.\"\"\"\n",
    "    type: str = Field(description=\"Type of format issue\")\n",
    "    message: str = Field(description=\"Short explanation\")\n",
    "    location: str = Field(description=\"Verb phrase ID, question ID, or entity ID\")\n",
    "\n",
    "\n",
    "class Judge_Format(BaseModel):\n",
    "    \"\"\"LLM-as-a-judge evaluation format for GSW comparisons.\"\"\"\n",
    "    overall_score: float = Field(description=\"Overall score between 0 and 100\")\n",
    "    usable_for_QA: bool = Field(description=\"True if the GSW is usable for QA, False otherwise\")\n",
    "    subscores: Subscores = Field(description=\"Subscores for coverage, precision, and format compliance\")\n",
    "    counts: Counts = Field(description=\"Counts of gold and predicted entities, facts, and violations\")\n",
    "    critical_violations: List[CriticalViolation] = Field(description=\"List of critical violations\", default_factory=list)\n",
    "    entity_alignment: List[EntityAlignment] = Field(description=\"List of entity alignments\", default_factory=list)\n",
    "    missing_entities: List[MissingEntity] = Field(description=\"List of missing entities\", default_factory=list)\n",
    "    extra_entities: List[ExtraEntity] = Field(description=\"List of extra entities\", default_factory=list)\n",
    "    fact_comparison: FactComparison = Field(description=\"Comparison of facts between gold and predicted GSW\")\n",
    "    format_issues: List[FormatIssue] = Field(description=\"List of format issues\", default_factory=list)\n",
    "    improvement_suggestions: List[str] = Field(description=\"List of improvement suggestions\", default_factory=list)\n",
    "\n",
    "    model_config = {\"extra\": \"forbid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78da9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_client = OpenAI(api_key=\"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\")\n",
    "all_judgements = []\n",
    "for i in range(len(test_musique_corpus_sample)):\n",
    "    outputs = judge_client.chat.completions.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": LLM_AS_A_JUDGE_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": LLM_AS_A_JUDGE_PROMPT.format(input_text=test_musique_corpus_sample[0], gold_gsw_json=pred_gsws[0], pred_gsw_json=golden_gsws[0])},\n",
    "    ],\n",
    "    response_format=Judge_Format,\n",
    ")\n",
    "    all_judgements.append(Judge_Format.model_validate_json(outputs.choices[0].message.content)[\"overall_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37645c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Judge_Format.model_validate_json(outputs.choices[0].message.content).model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59q5iw4trf3",
   "metadata": {},
   "source": [
    "# LoRA/DoRA Fine-Tuning for GSW Creation\n",
    "\n",
    "This section implements LoRA/DoRA fine-tuning to train a model on GSW creation using thinking mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t8o1vhufnv9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for LoRA training\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtiagt1unv",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Match Musique corpus documents to golden GSWs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vzpht43hmd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match documents to golden GSWs for training\n",
    "# For this example, we'll use the first 100 samples\n",
    "\n",
    "matched_pairs = []\n",
    "for i, (doc, gsw) in enumerate(zip(test_musique_corpus_sample[:100], golden_gsws[:100])):\n",
    "    matched_pairs.append({\n",
    "        \"global_id\": doc[\"global_id\"],\n",
    "        \"document\": doc,\n",
    "        \"gsw\": gsw\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(matched_pairs)} document-GSW pairs for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml5gsqjuvw",
   "metadata": {},
   "source": [
    "## Create HuggingFace Dataset\n",
    "\n",
    "Format the training data as chat messages for LoRA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0hfpgfv43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_messages(example):\n",
    "    \"\"\"Convert document-GSW pair to chat format.\"\"\"\n",
    "    document = example['document']\n",
    "    gsw = example['gsw']\n",
    "    \n",
    "    # Serialize GSW to JSON\n",
    "    if isinstance(gsw, GSWStructure):\n",
    "        assistant_response = gsw.model_dump_json(indent=4)\n",
    "    else:\n",
    "        assistant_response = json.dumps(gsw, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    # Create prompts using FactualExtractionPrompts\n",
    "    system_prompt = FactualExtractionPrompts.SYSTEM_PROMPT\n",
    "    user_prompt = FactualExtractionPrompts.USER_PROMPT_TEMPLATE.format(\n",
    "        input_text=document['text'],\n",
    "        background_context=\"\"\n",
    "    )\n",
    "    \n",
    "    # Create chat messages (model will add <think> tags during generation)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response},\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "raw_dataset = Dataset.from_list(matched_pairs)\n",
    "training_dataset = raw_dataset.map(\n",
    "    create_chat_messages,\n",
    "    remove_columns=raw_dataset.column_names,\n",
    "    desc=\"Creating chat-formatted training data\"\n",
    ")\n",
    "\n",
    "print(f\"Training dataset created with {len(training_dataset)} examples\")\n",
    "print(f\"Sample messages preview:\")\n",
    "print(json.dumps(training_dataset[0][\"messages\"][:2], indent=2)[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gvj38inlj08",
   "metadata": {},
   "source": [
    "## Configure Tokenizer\n",
    "\n",
    "Load and configure the tokenizer for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jie1puypgp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model settings\n",
    "MODEL_ID = \"Qwen/Qwen3-8B\"  # Change this to your desired model\n",
    "USE_DORA = False  # Set to True to use DoRA instead of LoRA\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Configure padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(f\"Tokenizer configured:\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"  Padding side: {tokenizer.padding_side}\")\n",
    "\n",
    "# Test chat template\n",
    "print(\"\\nTesting chat template...\")\n",
    "sample = training_dataset[0]\n",
    "formatted = tokenizer.apply_chat_template(\n",
    "    sample[\"messages\"],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "print(f\"Formatted text length: {len(formatted)}\")\n",
    "print(f\"First 500 chars: {formatted[:500]}\")\n",
    "print(f\"Last 200 chars: {formatted[-200:]}\")\n",
    "\n",
    "# Check for thinking mode\n",
    "if '<think>' in formatted or 'add_generation_prompt' in str(tokenizer.chat_template):\n",
    "    print(\"\\n✓ Thinking mode detected in template\")\n",
    "else:\n",
    "    print(\"\\n⚠ Note: Thinking tags not detected, model may add them during generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eozx77556av",
   "metadata": {},
   "source": [
    "## Configure LoRA/DoRA Training\n",
    "\n",
    "Set up the model, LoRA config, and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1go0eli4ln4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "OUTPUT_DIR = \"./gsw_creation_lora\"\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# LoRA/DoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=256,\n",
    "    lora_alpha=512,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora=USE_DORA,  # DoRA if True, LoRA if False\n",
    ")\n",
    "\n",
    "adapter_type = \"DoRA\" if USE_DORA else \"LoRA\"\n",
    "print(f\"Configured {adapter_type} with:\")\n",
    "print(f\"  r={lora_config.r}\")\n",
    "print(f\"  alpha={lora_config.lora_alpha}\")\n",
    "print(f\"  dropout={lora_config.lora_dropout}\")\n",
    "print(f\"  target_modules={lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vj69zuyzunh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6sk8flqp1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    report_to=\"none\",  # Change to \"wandb\" for W&B logging\n",
    ")\n",
    "\n",
    "print(f\"Training configuration ({adapter_type}):\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necbjcoi9x",
   "metadata": {},
   "source": [
    "## Initialize Trainer and Start Training\n",
    "\n",
    "Create the SFTTrainer and begin LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ryerjcy8lc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_function(example):\n",
    "    \"\"\"Format a single example using the tokenizer's chat template.\"\"\"\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "# Initialize SFTTrainer\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=training_dataset,\n",
    "    formatting_func=formatting_function,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting {adapter_type} training...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2l8ldjzbql",
   "metadata": {},
   "source": [
    "## Save the Trained Model\n",
    "\n",
    "Save the final LoRA adapters and optionally push to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pf6ds5lu5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "final_output_path = f\"{OUTPUT_DIR}/final\"\n",
    "print(f\"Saving final model to: {final_output_path}\")\n",
    "trainer.save_model(final_output_path)\n",
    "\n",
    "print(f\"\\n✓ Model saved successfully!\")\n",
    "print(f\"  Location: {final_output_path}\")\n",
    "print(f\"  Adapter type: {adapter_type}\")\n",
    "\n",
    "# Optional: Push to HuggingFace Hub\n",
    "# Uncomment and configure the following to push to Hub:\n",
    "# HUB_MODEL_ID = \"username/qwen3-gsw-creation-lora\"\n",
    "# print(f\"\\nPushing to HuggingFace Hub: {HUB_MODEL_ID}\")\n",
    "# trainer.push_to_hub(\n",
    "#     commit_message=f\"Training complete - {adapter_type} fine-tuned model for GSW creation\",\n",
    "#     blocking=True,\n",
    "# )\n",
    "# print(f\"✓ Model pushed to: https://huggingface.co/{HUB_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pfsg672njc",
   "metadata": {},
   "source": [
    "## Test the Fine-Tuned Model\n",
    "\n",
    "Test the trained model on a new document to verify it generates proper GSW structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05naw07y3kbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a sample document\n",
    "test_doc = test_musique_corpus_sample[0]\n",
    "\n",
    "# Create test prompt\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": FactualExtractionPrompts.SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": FactualExtractionPrompts.USER_PROMPT_TEMPLATE.format(\n",
    "        input_text=test_doc['text'],\n",
    "        background_context=\"\"\n",
    "    )}\n",
    "]\n",
    "\n",
    "# Format with chat template\n",
    "test_input = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "print(\"Generating GSW output...\")\n",
    "print(\"=\"*60)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        temperature=0.6,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "# Extract the assistant's response\n",
    "assistant_start = generated_text.rfind(\"<|im_start|>assistant\")\n",
    "if assistant_start != -1:\n",
    "    assistant_response = generated_text[assistant_start:]\n",
    "    print(\"Generated output:\")\n",
    "    print(assistant_response[:1000])  # Print first 1000 chars\n",
    "    print(\"\\n... [truncated] ...\")\n",
    "else:\n",
    "    print(\"Full generated text:\")\n",
    "    print(generated_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
