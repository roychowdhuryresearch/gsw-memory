{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bcc837",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from bespokelabs import curator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc522e",
   "metadata": {},
   "source": [
    "# Load Musique Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_musique = json.load(open(\"/home/yigit/codebase/gsw-memory/playground_data/musique.json\"))\n",
    "# Load Musique Train Data jsonl to json format\n",
    "train_musique = [json.loads(line) for line in open(\"/home/yigit/codebase/gsw-memory/playground_data/musique_full_v1.0_train.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_musique_questions = {q[\"id\"]: q[\"question\"] for q in test_musique}\n",
    "train_musique_questions = {q[\"id\"]: q[\"question\"] for q in train_musique}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare if there is any overlap between train and test questions\n",
    "set(train_musique_questions) & set(test_musique_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_musique_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print questions by type using train_musique_questions\n",
    "datapoint_type_counts = {}\n",
    "seen_ids = set()\n",
    "duplicate_ids = set()\n",
    "for qid in train_musique_questions:\n",
    "    # count by type\n",
    "    datapoint_type_counts[qid.split(\"_\")[0]] = datapoint_type_counts.get(qid.split(\"_\")[0], 0) + 1\n",
    "    # check for same id\n",
    "    if qid in seen_ids:\n",
    "        duplicate_ids.add(qid)\n",
    "    else:\n",
    "        seen_ids.add(qid)\n",
    "print(\"Datapoint type counts:\", datapoint_type_counts)\n",
    "if duplicate_ids:\n",
    "    print(\"Duplicate IDs found:\", duplicate_ids)\n",
    "else:\n",
    "    print(\"No duplicate IDs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 5 from each type from train_musique\n",
    "q_type_keys = list(datapoint_type_counts.keys())\n",
    "train_musique_questions_by_type = {q_type: [] for q_type in q_type_keys}\n",
    "for q_type in q_type_keys:\n",
    "    count = 0\n",
    "    for qid in train_musique_questions:\n",
    "        if qid.split(\"_\")[0] == q_type:\n",
    "            train_musique_questions_by_type[q_type].append(qid)\n",
    "            count += 1\n",
    "            if count == 500:\n",
    "                break\n",
    "            \n",
    "# print type length\n",
    "for q_type, qids in train_musique_questions_by_type.items():\n",
    "    print(f\"{q_type}: {len(qids)}\")\n",
    "    \n",
    "# convert train_musique_questions_by_type to flat list\n",
    "train_musique_questions_by_type_list = [q for q_type in q_type_keys for q in train_musique_questions_by_type[q_type]]\n",
    "len(train_musique_questions_by_type_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_inputs = [\n",
    "    {\"question_id\": qid, \"question\": train_musique_questions[qid]}\n",
    "    for qid in train_musique_questions_by_type_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(decompose_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207770a",
   "metadata": {},
   "source": [
    "# Define Question Decomposition Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposedQuestion(BaseModel):\n",
    "    question: str\n",
    "    requires_retrieval: bool\n",
    "\n",
    "class DecomposedQuestionList(BaseModel):\n",
    "    questions: List[DecomposedQuestion]\n",
    "\n",
    "class ChainQuestionDecomposer(curator.LLM):\n",
    "    \"\"\"Curator class for decomposing multi-hop questions in parallel.\"\"\"\n",
    "    \n",
    "    # return_completions_object = True\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize the question decomposer.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def prompt(self, input):\n",
    "        \"\"\"Create a decomposition prompt for each question.\"\"\"\n",
    "        decomposition_prompt = f\"\"\"Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
    "\n",
    "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
    "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure out how they connect.\n",
    "\n",
    "---\n",
    "## A Simple Analogy for Efficiency\n",
    "\n",
    "**Question:** \"What is the phone number of the mother of the tallest player on the Lakers?\"\n",
    "\n",
    "** Inefficient Path:**\n",
    "1.  Who are the players on the Lakers?\n",
    "2.  What are all their heights?\n",
    "3.  Who is the mother of the tallest player? *(This step is a logical leap)*\n",
    "\n",
    "** Efficient Path:**\n",
    "1.  Who is the tallest player on the Lakers?\n",
    "2.  Who is the mother of `<ENTITY_Q1>`?\n",
    "3.  What is the phone number of `<ENTITY_Q2>`?\n",
    "\n",
    "---\n",
    "## How to Decompose a Question\n",
    "This process follows a logical flow from high-level analysis to the fine-tuning of your question chain.\n",
    "\n",
    "### 1. Analyze the Query's Components\n",
    "First, break down the original question into its fundamental building blocks. Identify the core **entities** (people, places, organizations), their **properties** (attributes like rank, location, date), and the **relationships** that connect them.\n",
    "\n",
    "### 2. Construct an Atomic Chain\n",
    "Next, formulate a sequence of questions where each question retrieves a single fact.\n",
    "* **Isolate Comparisons:** Don't ask \"who is faster?\" Ask for the specific rank or time of each person involved.\n",
    "* **Link with Placeholders:** Use `<ENTITY_Qn>` to pass the answer from a previous question (`Qn`) into the next one.\n",
    "\n",
    "### 3. Optimize for Efficiency and Precision\n",
    "Your final goal is the **shortest and most direct path** to the answer.\n",
    "* **Embed Constraints to Build Bridges:** If a piece of information is only a filter (like a date or location), embed it as a constraint in the next question instead of asking for it directly.\n",
    "  **Important note for bridges:** There can be no `<ENTITY_Qn>` in the first question if the nth question DOES NOT require retrieval.\n",
    "\n",
    "## Formatting\n",
    "Format each decomposed question as follows:\n",
    "\n",
    "Question: [the question text]\n",
    "Requires retrieval: [true/false]\n",
    "\n",
    "And provide the response in the following JSON format:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"question\": \"the decomposed question text\",\n",
    "      \"requires_retrieval\": \"true/false\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Casablanca?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Input: \"Which film has the director who is older, Dune or The Dark Knight?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Dune?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who directed The Dark Knight?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": \"false\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "IMPORTANT:\n",
    "    AVOID over-decomposition like this:\n",
    "    DON'T break \"Who is John Doe?\" into:\n",
    "    1. Who is John Doe? → \"English\"\n",
    "    2. When was <ENTITY_Q1> born? → \"When was English born?\"\n",
    "\n",
    "    DO ask directly: \"When was John Doe born?\"\n",
    "\n",
    "Now decompose this question:\n",
    "Input: \"{input['question']}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that breaks down complex questions into simple steps.\"},\n",
    "            {\"role\": \"user\", \"content\": decomposition_prompt}\n",
    "        ]\n",
    "    \n",
    "    def parse(self, input, response: DecomposedQuestionList):\n",
    "        \"\"\"Parse the decomposition response.\"\"\"\n",
    "\n",
    "        # print(response)\n",
    "        questions = [{\"question\" : q.question, \"requires_retrieval\" : q.requires_retrieval} for q in response.questions]\n",
    "        \n",
    "        return [{\n",
    "            \"question_id\": input['question_id'],\n",
    "            \"original_question\": input['question'],\n",
    "            \"decomposed_questions\": questions,\n",
    "            # \"raw_response\": decomposition_text\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_question_decomposer = ChainQuestionDecomposer(\n",
    "                model_name=\"gpt-5\",\n",
    "                # model_name=\"gpt-4o\",\n",
    "                # generation_params={\"temperature\": 0.0}, \n",
    "                response_format=DecomposedQuestionList\n",
    "            )\n",
    "\n",
    "decomposition_dataset = golden_question_decomposer(decompose_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db609dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_results = {\n",
    "            item[\"question_id\"]: item\n",
    "            for item in decomposition_dataset.dataset\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_results\n",
    "# print the results as formatted JSON (without needing CustomJSONEncoder)\n",
    "print(json.dumps(decomposition_results, indent=4, ensure_ascii=False))\n",
    "with open('q_decomp_training_5_large.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(decomposition_results, f, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54254jyi6t",
   "metadata": {},
   "source": [
    "# Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bxr2mqh6q4u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert decomposition_results dict to a list\n",
    "decomposition_list = list(decomposition_results.values())\n",
    "\n",
    "print(f\"Total examples: {len(decomposition_list)}\")\n",
    "print(f\"\\nExample structure:\")\n",
    "print(f\"Keys: {decomposition_list[0].keys()}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"Question ID: {decomposition_list[0]['question_id']}\")\n",
    "print(f\"Original Question: {decomposition_list[0]['original_question']}\")\n",
    "print(f\"Decomposed Questions: {decomposition_list[0]['decomposed_questions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ekvzpdo5cds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create HuggingFace Dataset from the decomposition list\n",
    "raw_dataset = Dataset.from_list(decomposition_list)\n",
    "\n",
    "print(f\"Dataset info:\")\n",
    "print(raw_dataset)\n",
    "print(f\"\\nColumn names: {raw_dataset.column_names}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(raw_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gqptzsyqzh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_chat_messages(example):\n",
    "    \"\"\"\n",
    "    Convert a single example into chat format for training.\n",
    "    \n",
    "    Args:\n",
    "        example: Dict with 'original_question' and 'decomposed_questions' keys\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'messages' key containing the chat-formatted data\n",
    "    \"\"\"\n",
    "    original_question = example['original_question']\n",
    "    decomposed_questions = example['decomposed_questions']\n",
    "    \n",
    "    # Serialize the decomposed questions to JSON format (this is what the model should output)\n",
    "    assistant_response = json.dumps(\n",
    "        {\"questions\": decomposed_questions},\n",
    "        indent=4,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    "    \n",
    "    # Create the instruction prompt for the user\n",
    "    user_prompt = f\"\"\"Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
    "\n",
    "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
    "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure out how they connect.\n",
    "\n",
    "---\n",
    "## A Simple Analogy for Efficiency\n",
    "\n",
    "**Question:** \"What is the phone number of the mother of the tallest player on the Lakers?\"\n",
    "\n",
    "** Inefficient Path:**\n",
    "1.  Who are the players on the Lakers?\n",
    "2.  What are all their heights?\n",
    "3.  Who is the mother of the tallest player? *(This step is a logical leap)*\n",
    "\n",
    "** Efficient Path:**\n",
    "1.  Who is the tallest player on the Lakers?\n",
    "2.  Who is the mother of `<ENTITY_Q1>`?\n",
    "3.  What is the phone number of `<ENTITY_Q2>`?\n",
    "\n",
    "---\n",
    "## How to Decompose a Question\n",
    "This process follows a logical flow from high-level analysis to the fine-tuning of your question chain.\n",
    "\n",
    "### 1. Analyze the Query's Components\n",
    "First, break down the original question into its fundamental building blocks. Identify the core **entities** (people, places, organizations), their **properties** (attributes like rank, location, date), and the **relationships** that connect them.\n",
    "\n",
    "### 2. Construct an Atomic Chain\n",
    "Next, formulate a sequence of questions where each question retrieves a single fact.\n",
    "* **Isolate Comparisons:** Don't ask \"who is faster?\" Ask for the specific rank or time of each person involved.\n",
    "* **Link with Placeholders:** Use `<ENTITY_Qn>` to pass the answer from a previous question (`Qn`) into the next one.\n",
    "\n",
    "### 3. Optimize for Efficiency and Precision\n",
    "Your final goal is the **shortest and most direct path** to the answer.\n",
    "* **Embed Constraints to Build Bridges:** If a piece of information is only a filter (like a date or location), embed it as a constraint in the next question instead of asking for it directly.\n",
    "**Important note for bridges:** There can be no `<ENTITY_Qn>` in the first question if the nth question DOES NOT require retrieval.\n",
    "\n",
    "## Formatting\n",
    "Format each decomposed question as follows:\n",
    "\n",
    "<decomposition>\n",
    "Question: [the question text]\n",
    "Requires retrieval: [true/false]\n",
    "\n",
    "And provide the response in the following json format:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"question\": \"the decomposed question text\",\n",
    "      \"requires_retrieval\": \"true/false\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Casablanca?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Input: \"Which film has the director who is older, Dune or The Dark Knight?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Dune?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who directed The Dark Knight?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": \"false\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "IMPORTANT:\n",
    "    AVOID over-decomposition like this:\n",
    "    DON'T break \"Who is John Doe?\" into:\n",
    "    1. Who is John Doe? → \"English\"\n",
    "    2. When was <ENTITY_Q1> born? → \"When was English born?\"\n",
    "\n",
    "    DO ask directly: \"When was John Doe born?\"\n",
    "\n",
    "Now decompose this question:\n",
    "Input: \"{original_question}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    # Create the chat messages in the format expected by chat models\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response},\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "print(\"Preprocessing function created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xg2feiegi1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing to create the final training dataset\n",
    "training_dataset = raw_dataset.map(\n",
    "    create_chat_messages,\n",
    "    remove_columns=raw_dataset.column_names,  # Remove original columns, keep only 'messages'\n",
    "    desc=\"Creating chat-formatted training data\"\n",
    ")\n",
    "\n",
    "print(f\"Training dataset created!\")\n",
    "print(training_dataset)\n",
    "print(f\"\\nColumn names: {training_dataset.column_names}\")\n",
    "print(f\"\\nFirst example messages:\")\n",
    "print(f\"User message (first 500 chars): {training_dataset[0]['messages'][0]['content'][:500]}...\")\n",
    "print(f\"\\nAssistant response (first 500 chars): {training_dataset[0]['messages'][1]['content'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lqdzhwuxmib",
   "metadata": {},
   "source": [
    "# Test Formatting Function (Run Before Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vfvn8z9e3kc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the chat template works correctly before training\n",
    "print(\"Testing chat template compatibility...\")\n",
    "\n",
    "# Load tokenizer for testing\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# Configure padding\n",
    "if test_tokenizer.pad_token is None:\n",
    "    test_tokenizer.pad_token = test_tokenizer.eos_token\n",
    "    test_tokenizer.pad_token_id = test_tokenizer.eos_token_id\n",
    "\n",
    "test_tokenizer.padding_side = 'right'\n",
    "\n",
    "# Check if chat template exists\n",
    "if hasattr(test_tokenizer, 'chat_template') and test_tokenizer.chat_template:\n",
    "    print(\"✓ Chat template found!\")\n",
    "    print(f\"  Template preview (first 200 chars): {str(test_tokenizer.chat_template)[:200]}...\")\n",
    "else:\n",
    "    print(\"✗ WARNING: No chat template found! This may cause errors.\")\n",
    "    print(\"  Consider using an instruct-tuned model variant.\")\n",
    "\n",
    "# Test with a sample from the training dataset\n",
    "print(\"\\nTesting formatting with sample data...\")\n",
    "try:\n",
    "    sample = training_dataset[0]\n",
    "    print(f\"Sample messages structure: {list(sample.keys())}\")\n",
    "    \n",
    "    # Test the formatting function\n",
    "    formatted = test_tokenizer.apply_chat_template(\n",
    "        sample[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Formatting successful!\")\n",
    "    print(f\"  Original message length: {len(str(sample['messages']))}\")\n",
    "    print(f\"  Formatted text length: {len(formatted)}\")\n",
    "    print(f\"\\nFormatted output preview (first 500 chars):\")\n",
    "    print(formatted[:500])\n",
    "    print(\"\\n... [truncated] ...\")\n",
    "    print(f\"\\nLast 200 chars:\")\n",
    "    print(formatted[-200:])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR during formatting: {e}\")\n",
    "    print(\"  You may need to adjust the formatting function or use a different model.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test complete! Review the output above before training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dp56fjqa336",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning Setup for Local GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rjfv19zmp7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kp5fduzxpa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Model Loading and Training Loop Function (Local GPU Version)\n",
    "# =============================================================================\n",
    "def train(model_id, tokenizer, dataset, training_args):\n",
    "    \"\"\"\n",
    "    Train a model with LoRA on local GPU.\n",
    "    \n",
    "    Args:\n",
    "        model_id: HuggingFace model identifier (e.g., \"Qwen/Qwen3-8B\")\n",
    "        tokenizer: Tokenizer instance\n",
    "        dataset: Training dataset with 'messages' column\n",
    "        training_args: TrainingArguments instance\n",
    "    \"\"\"\n",
    "    dtype = torch.bfloat16 if training_args.bf16 else torch.float32\n",
    "    \n",
    "    # Load model for local GPU training\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
    "        # Optional: use 4-bit quantization to save memory (uncomment if needed)\n",
    "        # load_in_4bit=True,\n",
    "        # bnb_4bit_compute_dtype=dtype,\n",
    "        # bnb_4bit_use_double_quant=True,\n",
    "        # bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    \n",
    "    # LoRA configuration optimized for Qwen3\n",
    "    # Standard config targets attention and FFN layers\n",
    "    # Remove \"embed_tokens\" to avoid potential issues\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,  # LoRA rank - higher = more parameters but better adaptation\n",
    "        lora_alpha=128,  # Scaling factor (typically 2x rank)\n",
    "        lora_dropout=0.05,  # Dropout for LoRA layers\n",
    "        target_modules=[\n",
    "            # Attention layers\n",
    "            \"q_proj\",      # Query projection\n",
    "            \"k_proj\",      # Key projection\n",
    "            \"v_proj\",      # Value projection\n",
    "            \"o_proj\",      # Output projection\n",
    "            # Feed-forward network layers\n",
    "            \"gate_proj\",   # Gate projection\n",
    "            \"up_proj\",     # Up projection\n",
    "            \"down_proj\",   # Down projection\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    def formatting_function(example):\n",
    "        \"\"\"Format a single example using the tokenizer's chat template.\"\"\"\n",
    "        return tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "    \n",
    "    # Initialize SFTTrainer with LoRA\n",
    "    # Note: In newer versions of trl, use 'processing_class' instead of 'tokenizer'\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        processing_class=tokenizer,  # Use processing_class for newer trl versions\n",
    "        train_dataset=dataset,\n",
    "        formatting_func=formatting_function,  # Function to format each example\n",
    "        # max_seq_length=4096,  # Maximum sequence length for training\n",
    "        # packing=True,  # Pack multiple examples into one sequence for efficiency\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    trainer.save_model()\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Defining the script-specific arguments\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_id: str = field(\n",
    "        metadata={\"help\": \"The model that you want to train from the Hugging Face hub.\"},\n",
    "    )\n",
    "    output_dir: str = field(\n",
    "        default=\"./question_decomp_lora\",\n",
    "        metadata={\"help\": \"Directory to save the trained model.\"},\n",
    "    )\n",
    "    num_train_epochs: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of training epochs.\"},\n",
    "    )\n",
    "    per_device_train_batch_size: int = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Batch size per GPU for training.\"},\n",
    "    )\n",
    "    gradient_accumulation_steps: int = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Number of gradient accumulation steps.\"},\n",
    "    )\n",
    "    learning_rate: float = field(\n",
    "        default=2e-4,\n",
    "        metadata={\"help\": \"Learning rate for training.\"},\n",
    "    )\n",
    "    warmup_steps: int = field(\n",
    "        default=100,\n",
    "        metadata={\"help\": \"Number of warmup steps.\"},\n",
    "    )\n",
    "    logging_steps: int = field(\n",
    "        default=10,\n",
    "        metadata={\"help\": \"Log every N steps.\"},\n",
    "    )\n",
    "    save_steps: int = field(\n",
    "        default=500,\n",
    "        metadata={\"help\": \"Save checkpoint every N steps.\"},\n",
    "    )\n",
    "    save_total_limit: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Maximum number of checkpoints to keep.\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g80ydyiilg",
   "metadata": {},
   "source": [
    "# Example: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htgi8ri48wh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU visibility FIRST (before any CUDA operations)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\"\n",
    "\n",
    "# Verify GPU configuration\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of visible GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Example usage - Configure and run training\n",
    "\n",
    "# Configure model and tokenizer\n",
    "model_id = \"Qwen/Qwen3-8B\"  # Change to your desired Qwen model (or Qwen2.5-7B-Instruct, etc.)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Configure padding token properly\n",
    "# Many chat models don't have a pad token, so we use EOS as pad\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Important: Set padding side to 'right' for training (not inference)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(f\"\\nTokenizer configured:\")\n",
    "print(f\"  Model: {model_id}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"  Padding side: {tokenizer.padding_side}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./question_decomp_lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,  # Use bfloat16 if your GPU supports it\n",
    "    fp16=False,  # Use fp16 if bf16 is not supported\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",  # Change to \"wandb\" or \"tensorboard\" if you want logging\n",
    ")\n",
    "\n",
    "# Start training using the training_dataset created above\n",
    "# trainer = train(model_id, tokenizer, training_dataset, training_args)\n",
    "\n",
    "# After training, you can save and use the model:\n",
    "# trainer.save_model(\"./question_decomp_lora_final\")\n",
    "# Or push to HuggingFace Hub:\n",
    "# trainer.push_to_hub(\"your-username/question-decomp-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h174k0theho",
   "metadata": {},
   "source": [
    "# Load Trained LoRA Adapter for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474a5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jzn93nym6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen3-8B\n",
      "LoRA config: r=256, alpha=512, dropout=0.05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b846d67a58844967bf6a4afb28d3b025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "✓ Custom chat template loaded from qwen3_nonthinking.jinja\n",
      "✓ Tokenizer loaded successfully!\n",
      "  PAD token: <|endoftext|> (ID: 151643)\n",
      "  EOS token: <|im_end|> (ID: 151645)\n",
      "✓ Model set to evaluation mode\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# Path to your trained LoRA adapter\n",
    "adapter_path = \"/home/yigit/codebase/gsw-memory/question_decomp_lora/final\"\n",
    "\n",
    "# Load the PEFT config to get the base model name\n",
    "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "print(f\"Loading base model: {peft_config.base_model_name_or_path}\")\n",
    "print(f\"LoRA config: r={peft_config.r}, alpha={peft_config.lora_alpha}, dropout={peft_config.lora_dropout}\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load LoRA adapter on top of base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Merge adapter weights into base model for faster inference (optional)\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "\n",
    "# Load and apply the custom non-thinking chat template\n",
    "with open('/home/yigit/codebase/gsw-memory/playground/question_decomp_local/qwen3_nonthinking.jinja', 'r') as f:\n",
    "    custom_chat_template = f.read()\n",
    "\n",
    "tokenizer.chat_template = custom_chat_template\n",
    "print(\"✓ Custom chat template loaded from qwen3_nonthinking.jinja\")\n",
    "\n",
    "# Ensure padding is set correctly for inference\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"✓ Tokenizer loaded successfully!\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "print(\"✓ Model set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1876dfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5609e1e4904c15a2d491bc318bffff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "ADAPTER_PATH = \"/home/yigit/codebase/gsw-memory/question_decomp_lora/final\"\n",
    "MERGED_MODEL_PATH = \"Qwen3-8B-recipes-gpt5-large\"\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#change tokenizer chat template\n",
    "with open('/home/yigit/codebase/gsw-memory/playground/question_decomp_local/qwen3_nonthinking.jinja', 'r') as f:\n",
    "    custom_chat_template = f.read()\n",
    "\n",
    "tokenizer.chat_template = custom_chat_template\n",
    "\n",
    "# Load adapter configuration and model\n",
    "adapter_config = PeftConfig.from_pretrained(ADAPTER_PATH)\n",
    "finetuned_model = PeftModel.from_pretrained(model, ADAPTER_PATH, config=adapter_config)\n",
    "\n",
    "print(\"Saving tokenizer\")\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "print(\"Saving model\")\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "finetuned_model.save_pretrained(MERGED_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c853c107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59237ac3de04550bc57fd4fc062db48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbcc91334e645e8b6c69e1cbcd1492d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9393f25cfb43a08efde76e25895035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702c2d53a29c48fb8096d79c41125959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4efc5521e94a97be1c49222d6a93ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51621d3b467c4dcdbbcc8e812d40172d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yigitturali/qwen3-8b-qa-decomp-gsw-rank-256-gpt5-golden-large/commit/1c5b58c90864dbb876fa471aa0ff9f5f2fd54bc7', commit_message='Upload Qwen3ForCausalLM', commit_description='', oid='1c5b58c90864dbb876fa471aa0ff9f5f2fd54bc7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yigitturali/qwen3-8b-qa-decomp-gsw-rank-256-gpt5-golden-large', endpoint='https://huggingface.co', repo_type='model', repo_id='yigitturali/qwen3-8b-qa-decomp-gsw-rank-256-gpt5-golden-large'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MERGED_MODEL_PATH = \"Qwen3-8B-recipes-gpt5-large\"\n",
    "HUB_MODEL_NAME = \"yigitturali/qwen3-8b-qa-decomp-gsw-rank-256-gpt5-golden-large\"\n",
    "\n",
    "# Load and push tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "tokenizer.push_to_hub(HUB_MODEL_NAME)\n",
    "\n",
    "# Load and push model\n",
    "model = AutoModelForCausalLM.from_pretrained(MERGED_MODEL_PATH)\n",
    "model.push_to_hub(HUB_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "err0apziua4",
   "metadata": {},
   "source": [
    "# Fixed Inference Function (Handles `<think>` tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537r40ld9x",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decompose_question_fixed(question: str, temperature: float = 0.0, max_new_tokens: int = 1024) -> dict:\n",
    "    \"\"\"\n",
    "    Decompose a multi-hop question into atomic sub-questions using the fine-tuned LoRA model.\n",
    "    FIXED: Properly handles <think> tags and 'assistant' prefix from Qwen models.\n",
    "    \n",
    "    Args:\n",
    "        question: The multi-hop question to decompose\n",
    "        temperature: Sampling temperature (0.0 for greedy decoding)\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with original question and decomposed questions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the same prompt format used during training\n",
    "    user_prompt = f\"\"\"Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
    "\n",
    "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
    "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure out how they connect.\n",
    "\n",
    "---\n",
    "## A Simple Analogy for Efficiency\n",
    "\n",
    "**Question:** \"What is the phone number of the mother of the tallest player on the Lakers?\"\n",
    "\n",
    "** Inefficient Path:**\n",
    "1.  Who are the players on the Lakers?\n",
    "2.  What are all their heights?\n",
    "3.  Who is the mother of the tallest player? *(This step is a logical leap)*\n",
    "\n",
    "** Efficient Path:**\n",
    "1.  Who is the tallest player on the Lakers?\n",
    "2.  Who is the mother of `<ENTITY_Q1>`?\n",
    "3.  What is the phone number of `<ENTITY_Q2>`?\n",
    "\n",
    "---\n",
    "## How to Decompose a Question\n",
    "This process follows a logical flow from high-level analysis to the fine-tuning of your question chain.\n",
    "\n",
    "### 1. Analyze the Query's Components\n",
    "First, break down the original question into its fundamental building blocks. Identify the core **entities** (people, places, organizations), their **properties** (attributes like rank, location, date), and the **relationships** that connect them.\n",
    "\n",
    "### 2. Construct an Atomic Chain\n",
    "Next, formulate a sequence of questions where each question retrieves a single fact.\n",
    "* **Isolate Comparisons:** Don't ask \"who is faster?\" Ask for the specific rank or time of each person involved.\n",
    "* **Link with Placeholders:** Use `<ENTITY_Qn>` to pass the answer from a previous question (`Qn`) into the next one.\n",
    "\n",
    "### 3. Optimize for Efficiency and Precision\n",
    "Your final goal is the **shortest and most direct path** to the answer.\n",
    "* **Embed Constraints to Build Bridges:** If a piece of information is only a filter (like a date or location), embed it as a constraint in the next question instead of asking for it directly.\n",
    "**Important note for bridges:** There can be no `<ENTITY_Qn>` in the first question if the nth question DOES NOT require retrieval.\n",
    "\n",
    "## Formatting\n",
    "Format each decomposed question as follows:\n",
    "\n",
    "<decomposition>\n",
    "Question: [the question text]\n",
    "Requires retrieval: [true/false]\n",
    "\n",
    "And provide the response in the following json format:\n",
    "{{{{\n",
    "  \"questions\": [\n",
    "    {{{{\n",
    "      \"question\": \"the decomposed question text\",\n",
    "      \"requires_retrieval\": \"true/false\"\n",
    "    }}}}\n",
    "  ]\n",
    "}}}}\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "Output:\n",
    "{{{{\n",
    "    \"questions\": [\n",
    "        {{{{\n",
    "            \"question\": \"Who directed Casablanca?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}}},\n",
    "        {{{{\n",
    "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}}},\n",
    "        {{{{\n",
    "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}}}\n",
    "    ]\n",
    "}}}}\n",
    "\n",
    "Input: \"Which film has the director who is older, Dune or The Dark Knight?\"\n",
    "Output:\n",
    "{{{{\n",
    "    \"questions\": [\n",
    "        {{{{\n",
    "            \"question\": \"Who directed Dune?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}}},\n",
    "        {{{{\n",
    "            \"question\": \"Who directed The Dark Knight?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}}},\n",
    "        {{{{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": \"true\"\n",
    "        }}}},\n",
    "        {{{{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": \"false\"\n",
    "        }}}}\n",
    "    ]\n",
    "}}}}\n",
    "\n",
    "\n",
    "IMPORTANT:\n",
    "    AVOID over-decomposition like this:\n",
    "    DON'T break \"Who is John Doe?\" into:\n",
    "    1. Who is John Doe? → \"English\"\n",
    "    2. When was <ENTITY_Q1> born? → \"When was English born?\"\n",
    "\n",
    "    DO ask directly: \"When was John Doe born?\"\n",
    "\n",
    "Now decompose this question:\n",
    "Input: \"{question}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    # Format as chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template (uses the custom template loaded on the tokenizer)\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=temperature > 0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the assistant's response (after \"Output:\\n\")\n",
    "    if \"Output:\\n\" in generated_text:\n",
    "        assistant_response = generated_text.split(\"Output:\\n\")[-1].strip()\n",
    "    else:\n",
    "        # Fallback: take everything after the last occurrence of \"assistant\"\n",
    "        assistant_response = generated_text.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    # Remove \"assistant\" prefix if it still exists at the beginning\n",
    "    if assistant_response.lower().startswith(\"assistant\"):\n",
    "        assistant_response = assistant_response[len(\"assistant\"):].strip()\n",
    "    \n",
    "    # Remove the <think> tags if present (Qwen models use this for reasoning)\n",
    "    # Use regex to remove everything between <think> and </think> including the tags\n",
    "    assistant_response = re.sub(r'<think>.*?</think>', '', assistant_response, flags=re.DOTALL).strip()\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        parsed_json = json.loads(assistant_response)\n",
    "        return {\n",
    "            \"original_question\": question,\n",
    "            \"decomposed_questions\": parsed_json.get(\"questions\", []),\n",
    "            \"raw_response\": assistant_response,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            \"original_question\": question,\n",
    "            \"decomposed_questions\": [],\n",
    "            \"raw_response\": assistant_response,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"✓ Fixed inference function created successfully!\")\n",
    "print(\"  This version uses the custom chat template set on the tokenizer\")\n",
    "print(\"  It properly strips <think> tags and 'assistant' prefix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3tz2ueoio",
   "metadata": {},
   "source": [
    "# Apply Non-Thinking Chat Template for Re-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5wb6i8inf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the non-thinking chat template\n",
    "with open('/home/yigit/codebase/gsw-memory/qwen3_nonthinking.jinja', 'r') as f:\n",
    "    non_thinking_template = f.read()\n",
    "\n",
    "print(\"Original template (lines 84-86 with add_generation_prompt):\")\n",
    "print(non_thinking_template.split('\\n')[83:86])\n",
    "print()\n",
    "\n",
    "# Create a truly non-thinking version by removing the <think> tags from generation prompt\n",
    "non_thinking_template_fixed = non_thinking_template.replace(\n",
    "    \"{{- '<|im_start|>assistant\\\\n<think>\\\\n\\\\n</think>\\\\n\\\\n' }}\",\n",
    "    \"{{- '<|im_start|>assistant\\\\n' }}\"\n",
    ")\n",
    "\n",
    "# Also remove the think tag handling from assistant messages (lines 44-45)\n",
    "# This ensures training data doesn't include think tags either\n",
    "non_thinking_template_fixed = non_thinking_template_fixed.replace(\n",
    "    \"{{- '<|im_start|>' + message.role + '\\\\n<think>\\\\n' + reasoning_content.strip('\\\\n') + '\\\\n</think>\\\\n\\\\n' + content.lstrip('\\\\n') }}\",\n",
    "    \"{{- '<|im_start|>' + message.role + '\\\\n' + content.lstrip('\\\\n') }}\"\n",
    ")\n",
    "\n",
    "print(\"Modified template - removed <think> tags from generation prompt\")\n",
    "print()\n",
    "\n",
    "# Apply the fixed template to a new tokenizer for training\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load fresh tokenizer\n",
    "tokenizer_no_think = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# Apply the modified chat template\n",
    "tokenizer_no_think.chat_template = non_thinking_template_fixed\n",
    "\n",
    "# Configure padding\n",
    "if tokenizer_no_think.pad_token is None:\n",
    "    tokenizer_no_think.pad_token = tokenizer_no_think.eos_token\n",
    "    tokenizer_no_think.pad_token_id = tokenizer_no_think.eos_token_id\n",
    "\n",
    "tokenizer_no_think.padding_side = 'right'\n",
    "\n",
    "print(\"✓ Non-thinking tokenizer created!\")\n",
    "print()\n",
    "\n",
    "# Test the template\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"4\"}\n",
    "]\n",
    "\n",
    "formatted_train = tokenizer_no_think.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "formatted_infer = tokenizer_no_think.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Test formatting (training - should have NO <think> tags):\")\n",
    "print(formatted_train)\n",
    "print()\n",
    "print(\"Test formatting (inference - should have NO <think> tags):\")\n",
    "print(formatted_infer)\n",
    "print()\n",
    "\n",
    "# Check if think tags are present\n",
    "if '<think>' in formatted_train or '<think>' in formatted_infer:\n",
    "    print(\"⚠ WARNING: <think> tags still present!\")\n",
    "else:\n",
    "    print(\"✓ SUCCESS: No <think> tags in formatted output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ekshh9g21s",
   "metadata": {},
   "source": [
    "# Re-train with Non-Thinking Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ygv7u44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU visibility FIRST (before any CUDA operations)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\"\n",
    "\n",
    "# Verify GPU configuration\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of visible GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Configuration with Non-Thinking Template\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the tokenizer with non-thinking template created above\n",
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "print(f\"\\nTokenizer configured:\")\n",
    "print(f\"  Model: {model_id}\")\n",
    "print(f\"  EOS token: {tokenizer_no_think.eos_token} (ID: {tokenizer_no_think.eos_token_id})\")\n",
    "print(f\"  PAD token: {tokenizer_no_think.pad_token} (ID: {tokenizer_no_think.pad_token_id})\")\n",
    "print(f\"  BOS token: {tokenizer_no_think.bos_token} (ID: {tokenizer_no_think.bos_token_id})\")\n",
    "print(f\"  Padding side: {tokenizer_no_think.padding_side}\")\n",
    "print(f\"  Chat template: Modified to remove <think> tags\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./question_decomp_lora_no_think\",  # Different output dir\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,  # Use bfloat16 if your GPU supports it\n",
    "    fp16=False,  # Use fp16 if bf16 is not supported\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_dir=\"./logs_no_think\",\n",
    "    report_to=\"none\",  # Change to \"wandb\" or \"tensorboard\" if you want logging\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining args:\")\n",
    "print(f\"  Output dir: {training_args.output_dir}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready to train! Uncomment the line below to start training:\")\n",
    "print(\"=\"*80)\n",
    "print(\"# trainer = train(model_id, tokenizer_no_think, training_dataset, training_args)\")\n",
    "print()\n",
    "\n",
    "# Uncomment to start training:\n",
    "# trainer = train(model_id, tokenizer_no_think, training_dataset, training_args)\n",
    "\n",
    "# After training completes, save the model:\n",
    "# trainer.save_model(\"./question_decomp_lora_no_think_final\")\n",
    "# tokenizer_no_think.save_pretrained(\"./question_decomp_lora_no_think_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulazd26k2wr",
   "metadata": {},
   "source": [
    "# Verify Template Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ra2iwfzmrfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the outputs from original vs non-thinking tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load original Qwen3 tokenizer (with default template)\n",
    "tokenizer_original = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: Original vs Non-Thinking Template\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Format with original tokenizer\n",
    "formatted_original = tokenizer_original.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Format with non-thinking tokenizer\n",
    "formatted_no_think = tokenizer_no_think.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"\\n1. ORIGINAL TEMPLATE OUTPUT (with <think> tags):\")\n",
    "print(\"-\" * 80)\n",
    "print(formatted_original)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n2. NON-THINKING TEMPLATE OUTPUT (NO <think> tags):\")\n",
    "print(\"-\" * 80)\n",
    "print(formatted_no_think)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n3. DIFFERENCE:\")\n",
    "print(\"-\" * 80)\n",
    "if '<think>' in formatted_original and '<think>' not in formatted_no_think:\n",
    "    print(\"✓ SUCCESS: Original has <think> tags, Non-thinking does NOT\")\n",
    "    print(f\"\\nOriginal ends with: ...{formatted_original[-100:]}\")\n",
    "    print(f\"\\nNon-thinking ends with: ...{formatted_no_think[-100:]}\")\n",
    "else:\n",
    "    print(\"⚠ Something unexpected happened\")\n",
    "    print(f\"Original has <think>: {'<think>' in formatted_original}\")\n",
    "    print(f\"Non-thinking has <think>: {'<think>' in formatted_no_think}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"This confirms the non-thinking template will NOT generate <think> tags!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mh0oqrs7uwq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed function\n",
    "test_question = \"What year did the writer of Crazy Little Thing Called Love die?\"\n",
    "\n",
    "print(f\"Testing fixed function with: {test_question}\\n\")\n",
    "print(\"Generating decomposition...\")\n",
    "\n",
    "result = decompose_question_fixed(test_question, temperature=0.0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULT WITH FIXED FUNCTION:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"\\nOriginal Question: {result['original_question']}\")\n",
    "\n",
    "if result['success']:\n",
    "    print(f\"\\nDecomposed Questions:\")\n",
    "    for i, q in enumerate(result['decomposed_questions'], 1):\n",
    "        retrieval_marker = \"🔍\" if q['requires_retrieval'] else \"💭\"\n",
    "        print(f\"  {retrieval_marker} Q{i}: {q['question']}\")\n",
    "        print(f\"      Requires retrieval: {q['requires_retrieval']}\")\n",
    "    \n",
    "    print(f\"\\n✓ JSON parsing successful!\")\n",
    "    print(f\"Raw JSON response:\\n{result['raw_response']}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Error: {result.get('error', 'Unknown error')}\")\n",
    "    print(f\"\\nRaw response:\\n{result['raw_response']}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f485679",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fip1ztl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple questions from the MuSiQue test set\n",
    "test_questions = [\n",
    "    \"Who succeeded the first President of Namibia?\",\n",
    "    \"What currency is used where Billy Giles died?\",\n",
    "    \"When was the first establishment that Mc-Donaldization is named after, open in the country Horndean is located?\",\n",
    "    \"When did Napoleon occupy the city where the mother of the woman who brought Louis XVI style to the court died?\",\n",
    "    \"How many Germans live in the colonial holding in Aruba’s continent that was governed by Prazeres’s country?\",\n",
    "    \"When did the people who captured Malakoff come to the region where Philipsburg is located?\"\n",
    "]\n",
    "\n",
    "print(\"Testing multiple questions from various domains...\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i}/{len(test_questions)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    result = decompose_question_fixed(question, temperature=0.0)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"✓ Successfully decomposed!\\n\")\n",
    "        print(\"Decomposition:\")\n",
    "        for j, q in enumerate(result['decomposed_questions'], 1):\n",
    "            retrieval_marker = \"🔍\" if q['requires_retrieval'] else \"💭\"\n",
    "            print(f\"  {retrieval_marker} Q{j}: {q['question']}\")\n",
    "    else:\n",
    "        print(f\"✗ Failed to parse JSON\")\n",
    "        print(f\"Error: {result.get('error', 'Unknown')}\")\n",
    "        print(f\"Raw response: {result['raw_response'][:200]}...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
