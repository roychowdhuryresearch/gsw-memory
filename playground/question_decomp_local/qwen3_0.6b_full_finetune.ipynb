{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iajq1W8ipjyK"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "5b6ad839-3f96-4d4e-d7a7-93af885545e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yigit/anaconda3/envs/model_trainer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.2: Fast Qwen3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 4. Max memory: 47.402 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%.\n",
      "To enable float32 training, use `float32_mixed_precision = True` during FastLanguageModel.from_pretrained\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "# FULL FINETUNING MODE - Using Qwen3-0.6B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-0.6B\",\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = False,    # Must be False for full finetuning\n",
    "    load_in_8bit = False,    # Must be False for full finetuning - will use BF16\n",
    "    full_finetuning = True,  # ENABLED: Full finetuning instead of LoRA!\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "**FULL FINETUNING MODE** - We are NOT using LoRA adapters. All model parameters will be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6bZsfBuZDeCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with full finetuning enabled.\n",
      "Total parameters: 596,049,920\n",
      "Trainable parameters: 596,049,920\n"
     ]
    }
   ],
   "source": [
    "# FULL FINETUNING - Skip LoRA adapter setup\n",
    "# For full finetuning, we train all parameters directly without LoRA adapters\n",
    "# The model is already prepared for full finetuning from the previous cell\n",
    "\n",
    "print(f\"Model loaded with full finetuning enabled.\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep - Question Decomposition Dataset\n",
    "\n",
    "We use a custom dataset of question decompositions for training. The dataset contains:\n",
    "- Original multi-hop questions\n",
    "- Decomposed single-hop questions with retrieval flags\n",
    "- Each example is formatted as a user-assistant conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5kyTw2n1edte"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /home/yigit/codebase/gsw-memory/playground/question_decomp_local/q_decomp_training_5_large.json\n",
      "Loaded 2527 question decompositions\n",
      "\n",
      "Example structure:\n",
      "Keys: dict_keys(['question_id', 'original_question', 'decomposed_questions'])\n",
      "\n",
      "First example:\n",
      "Question ID: 2hop__42543_20093\n",
      "Original Question: What year did the writer of Crazy Little Thing Called Love die?\n",
      "Decomposed Questions: [{'question': 'Who wrote the song \"Crazy Little Thing Called Love\"?', 'requires_retrieval': True}, {'question': 'In what year did <ENTITY_Q1> die?', 'requires_retrieval': True}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the question decomposition dataset\n",
    "dataset_path = \"/home/yigit/codebase/gsw-memory/playground/question_decomp_local/q_decomp_training_5_large.json\"\n",
    "\n",
    "print(f\"Loading dataset from: {dataset_path}\")\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    decomposition_results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(decomposition_results)} question decompositions\")\n",
    "\n",
    "# Convert to list format\n",
    "decomposition_list = list(decomposition_results.values())\n",
    "\n",
    "print(f\"\\nExample structure:\")\n",
    "print(f\"Keys: {decomposition_list[0].keys()}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"Question ID: {decomposition_list[0]['question_id']}\")\n",
    "print(f\"Original Question: {decomposition_list[0]['original_question']}\")\n",
    "print(f\"Decomposed Questions: {decomposition_list[0]['decomposed_questions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTZICZtie3lQ"
   },
   "source": [
    "### Convert to Chat Format\n",
    "\n",
    "Now we'll convert each example into chat format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DjgH3lt0e2Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat formatting function created!\n"
     ]
    }
   ],
   "source": [
    "def create_chat_messages(example):\n",
    "    \"\"\"\n",
    "    Convert a single example into chat format for training.\n",
    "\n",
    "    Args:\n",
    "        example: Dict with 'original_question' and 'decomposed_questions' keys\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'messages' key containing the chat-formatted data\n",
    "    \"\"\"\n",
    "    original_question = example['original_question']\n",
    "    decomposed_questions = example['decomposed_questions']\n",
    "\n",
    "    # Serialize the decomposed questions to JSON format (this is what the model should output)\n",
    "    assistant_response = json.dumps(\n",
    "        {\"questions\": decomposed_questions},\n",
    "        indent=4,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    "\n",
    "    # Create the instruction prompt for the user (same as used in question_decomp_lora_ft.py)\n",
    "    user_prompt = f\"\"\"Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
    "\n",
    "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
    "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure out how they connect.\n",
    "\n",
    "---\n",
    "## A Simple Analogy for Efficiency\n",
    "\n",
    "**Question:** \"What is the phone number of the mother of the tallest player on the Lakers?\"\n",
    "\n",
    "** Inefficient Path:**\n",
    "1.  Who are the players on the Lakers?\n",
    "2.  What are all their heights?\n",
    "3.  Who is the mother of the tallest player? *(This step is a logical leap)*\n",
    "\n",
    "** Efficient Path:**\n",
    "1.  Who is the tallest player on the Lakers?\n",
    "2.  Who is the mother of `<ENTITY_Q1>`?\n",
    "3.  What is the phone number of `<ENTITY_Q2>`?\n",
    "\n",
    "---\n",
    "## How to Decompose a Question\n",
    "This process follows a logical flow from high-level analysis to the fine-tuning of your question chain.\n",
    "\n",
    "### 1. Analyze the Query's Components\n",
    "First, break down the original question into its fundamental building blocks. Identify the core **entities** (people, places, organizations), their **properties** (attributes like rank, location, date), and the **relationships** that connect them.\n",
    "\n",
    "### 2. Construct an Atomic Chain\n",
    "Next, formulate a sequence of questions where each question retrieves a single fact.\n",
    "* **Isolate Comparisons:** Don't ask \"who is faster?\" Ask for the specific rank or time of each person involved.\n",
    "* **Link with Placeholders:** Use `<ENTITY_Qn>` to pass the answer from a previous question (`Qn`) into the next one.\n",
    "\n",
    "### 3. Optimize for Efficiency and Precision\n",
    "Your final goal is the **shortest and most direct path** to the answer.\n",
    "* **Embed Constraints to Build Bridges:** If a piece of information is only a filter (like a date or location), embed it as a constraint in the next question instead of asking for it directly.\n",
    "**Important note for bridges:** There can be no `<ENTITY_Qn>` in the first question if the nth question DOES NOT require retrieval.\n",
    "\n",
    "## Formatting\n",
    "Format each decomposed question as follows:\n",
    "\n",
    "Question: [the question text]\n",
    "Requires retrieval: [true/false]\n",
    "\n",
    "And provide the response in the following JSON format:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"question\": \"the decomposed question text\",\n",
    "      \"requires_retrieval\": \"true/false\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Casablanca?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Input: \"Which film has the director who is older, Dune or The Dark Knight?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Dune?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who directed The Dark Knight?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": False\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "IMPORTANT:\n",
    "    AVOID over-decomposition like this:\n",
    "    DON'T break \"Who is John Doe?\" into:\n",
    "    1. Who is John Doe? ‚Üí \"English\"\n",
    "    2. When was <ENTITY_Q1> born? ‚Üí \"When was English born?\"\n",
    "\n",
    "    DO ask directly: \"When was John Doe born?\"\n",
    "\n",
    "Now decompose this question:\n",
    "Input: \"{original_question}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "    # Create the chat messages in the format expected by chat models\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response},\n",
    "    ]\n",
    "\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "print(\"Chat formatting function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_zoaygOAe3I2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset info:\n",
      "Dataset({\n",
      "    features: ['question_id', 'original_question', 'decomposed_questions'],\n",
      "    num_rows: 2527\n",
      "})\n",
      "\n",
      "Column names: ['question_id', 'original_question', 'decomposed_questions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chat-formatted training data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2527/2527 [00:00<00:00, 7806.22 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training dataset created!\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 2527\n",
      "})\n",
      "\n",
      "Column names: ['messages']\n",
      "\n",
      "First example messages (user prompt first 500 chars):\n",
      "Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
      "\n",
      "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
      "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure o...\n",
      "\n",
      "Assistant response (first 300 chars):\n",
      "{\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"question\": \"Who wrote the song \\\"Crazy Little Thing Called Love\\\"?\",\n",
      "            \"requires_retrieval\": true\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"In what year did <ENTITY_Q1> die?\",\n",
      "            \"requires_retrieval\": true\n",
      "        }\n",
      "    ]\n",
      "}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create HuggingFace Dataset from the decomposition list\n",
    "raw_dataset = Dataset.from_list(decomposition_list)\n",
    "\n",
    "print(f\"Raw dataset info:\")\n",
    "print(raw_dataset)\n",
    "print(f\"\\nColumn names: {raw_dataset.column_names}\")\n",
    "\n",
    "# Apply the preprocessing to create the final training dataset\n",
    "training_dataset = raw_dataset.map(\n",
    "    create_chat_messages,\n",
    "    remove_columns=raw_dataset.column_names,  # Remove original columns, keep only 'messages'\n",
    "    desc=\"Creating chat-formatted training data\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining dataset created!\")\n",
    "print(training_dataset)\n",
    "print(f\"\\nColumn names: {training_dataset.column_names}\")\n",
    "print(f\"\\nFirst example messages (user prompt first 500 chars):\")\n",
    "print(training_dataset[0]['messages'][0]['content'][:500] + \"...\")\n",
    "print(f\"\\nAssistant response (first 300 chars):\")\n",
    "print(training_dataset[0]['messages'][1]['content'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1hUgF8LLjXt"
   },
   "source": [
    "### Train/Eval Split (600/200)\n",
    "\n",
    "We'll split the dataset stratified by question type to maintain the distribution across train and eval sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RKoBPQ7zLjXt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question type distribution:\n",
      "  2hop: 500\n",
      "  3hop1: 500\n",
      "  3hop2: 500\n",
      "  4hop1: 500\n",
      "  4hop2: 127\n",
      "  4hop3: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2527/2527 [00:00<00:00, 284911.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Split complete!\n",
      "  Training set: 2027 examples\n",
      "  Evaluation set: 500 examples\n",
      "\n",
      "Training set distribution:\n",
      "  2hop: 401\n",
      "  3hop1: 401\n",
      "  3hop2: 401\n",
      "  4hop1: 401\n",
      "  4hop2: 102\n",
      "  4hop3: 321\n",
      "\n",
      "Evaluation set distribution:\n",
      "  2hop: 99\n",
      "  3hop1: 99\n",
      "  3hop2: 99\n",
      "  4hop1: 99\n",
      "  4hop2: 25\n",
      "  4hop3: 79\n"
     ]
    }
   ],
   "source": [
    "# Split dataset: 600 train / 200 eval (stratified by question type)\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel\n",
    "\n",
    "# Extract question types from IDs (e.g., \"2hop\", \"3hop1\", \"4hop2\")\n",
    "def get_question_type(question_id):\n",
    "    \"\"\"Extract question type from ID for stratification.\"\"\"\n",
    "    return question_id.split(\"__\")[0]\n",
    "\n",
    "# Add question types to raw dataset\n",
    "decomposition_list_with_types = [\n",
    "    {**item, \"question_type\": get_question_type(item[\"question_id\"])}\n",
    "    for item in decomposition_list\n",
    "]\n",
    "\n",
    "# Count distribution\n",
    "type_counts = Counter([item[\"question_type\"] for item in decomposition_list_with_types])\n",
    "print(\"Question type distribution:\")\n",
    "for qtype, count in sorted(type_counts.items()):\n",
    "    print(f\"  {qtype}: {count}\")\n",
    "\n",
    "# Create dataset with question types\n",
    "full_dataset = Dataset.from_list(decomposition_list_with_types)\n",
    "\n",
    "# Convert question_type to ClassLabel for stratification\n",
    "unique_types = sorted(list(set([item[\"question_type\"] for item in decomposition_list_with_types])))\n",
    "full_dataset = full_dataset.cast_column(\n",
    "    \"question_type\",\n",
    "    ClassLabel(names=unique_types)\n",
    ")\n",
    "\n",
    "# Stratified split\n",
    "split_dataset = full_dataset.train_test_split(\n",
    "    test_size=500,\n",
    "    train_size=2027,\n",
    "    stratify_by_column=\"question_type\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_raw = split_dataset[\"train\"]\n",
    "eval_raw = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"\\n‚úì Split complete!\")\n",
    "print(f\"  Training set: {len(train_raw)} examples\")\n",
    "print(f\"  Evaluation set: {len(eval_raw)} examples\")\n",
    "\n",
    "# Verify stratification\n",
    "train_types = Counter([unique_types[idx] for idx in train_raw[\"question_type\"]])\n",
    "eval_types = Counter([unique_types[idx] for idx in eval_raw[\"question_type\"]])\n",
    "\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "for qtype in unique_types:\n",
    "    print(f\"  {qtype}: {train_types[qtype]}\")\n",
    "\n",
    "print(f\"\\nEvaluation set distribution:\")\n",
    "for qtype in unique_types:\n",
    "    print(f\"  {qtype}: {eval_types[qtype]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CijCaQPDLjXt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chat-formatted training data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2027/2027 [00:00<00:00, 9392.56 examples/s]\n",
      "Creating chat-formatted evaluation data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 11230.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chat-formatted datasets ready:\n",
      "  Training: 2027 examples\n",
      "  Evaluation: 500 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply chat formatting to both train and eval sets\n",
    "training_dataset = train_raw.map(\n",
    "    create_chat_messages,\n",
    "    remove_columns=train_raw.column_names,\n",
    "    desc=\"Creating chat-formatted training data\"\n",
    ")\n",
    "\n",
    "eval_dataset = eval_raw.map(\n",
    "    create_chat_messages,\n",
    "    remove_columns=eval_raw.column_names,\n",
    "    desc=\"Creating chat-formatted evaluation data\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Chat-formatted datasets ready:\")\n",
    "print(f\"  Training: {len(training_dataset)} examples\")\n",
    "print(f\"  Evaluation: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX8H3urDe00l"
   },
   "source": [
    "### Apply Chat Template\n",
    "\n",
    "Convert messages to text format using the tokenizer's chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing chat template formatting...\n",
      "\n",
      "Formatted sample (first 500 chars):\n",
      "<|im_start|>user\n",
      "Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
      "\n",
      "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
      "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then t\n",
      "\n",
      "... [content truncated] ...\n",
      "\n",
      "Last 200 chars:\n",
      "           \"requires_retrieval\": true\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How many undergraduates does <ENTITY_Q3> have?\",\n",
      "            \"requires_retrieval\": true\n",
      "        }\n",
      "    ]\n",
      "}<|im_end|>\n",
      "\n",
      "\n",
      "‚úì Chat template formatting works!\n",
      "Training dataset ready with 2027 examples\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template to convert to final training format\n",
    "# This will be done automatically by SFTTrainer, but we test it here\n",
    "\n",
    "print(\"Testing chat template formatting...\")\n",
    "sample_formatted = tokenizer.apply_chat_template(\n",
    "    training_dataset[0][\"messages\"],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(f\"\\nFormatted sample (first 500 chars):\")\n",
    "print(sample_formatted[:500])\n",
    "print(\"\\n... [content truncated] ...\")\n",
    "print(f\"\\nLast 200 chars:\")\n",
    "print(sample_formatted[-200:])\n",
    "\n",
    "print(f\"\\n‚úì Chat template formatting works!\")\n",
    "print(f\"Training dataset ready with {len(training_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFzVcXRcLjXu"
   },
   "source": [
    "### LLM Judge for Evaluation\n",
    "\n",
    "We'll use GPT-5 (with reasoning) to evaluate question decomposition quality every 3 training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2FhXqb2fLjXu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì QuestionDecompositionJudge class defined\n"
     ]
    }
   ],
   "source": [
    "# LLM Judge Implementation\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class QuestionDecompositionJudge:\n",
    "    \"\"\"GPT-5 based judge for evaluating question decomposition quality.\"\"\"\n",
    "\n",
    "    def __init__(self, model=\"gpt-5\", temperature=0.0):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def create_judge_prompt(self, original_question: str, decomposed_questions: List[Dict]) -> str:\n",
    "        \"\"\"Create evaluation prompt for the judge.\"\"\"\n",
    "        decomp_str = json.dumps({\"questions\": decomposed_questions}, indent=2)\n",
    "\n",
    "        return f\"\"\"You are an expert evaluator of question decomposition quality for multi-hop QA systems.\n",
    "\n",
    "**Original Question:** {original_question}\n",
    "\n",
    "**Generated Decomposition:**\n",
    "{decomp_str}\n",
    "\n",
    "**Evaluation Criteria** (Score each decomposed question 1-5):\n",
    "\n",
    "1. **Atomicity**: Is this a single-hop question retrieving only one piece of information?\n",
    "2. **Bridge Building**: Proper use of <ENTITY_Qn> placeholders to reference previous answers?\n",
    "3. **Efficiency**: Most direct path, avoiding over-decomposition?\n",
    "4. **Correctness**: Logically sound and contributes to answering the original question?\n",
    "5. **Retrieval Flag**: Is requires_retrieval set correctly?\n",
    "\n",
    "**Response Format (JSON):**\n",
    "{{\n",
    "  \"evaluations\": [\n",
    "    {{\n",
    "      \"question_index\": 0,\n",
    "      \"question_text\": \"...\",\n",
    "      \"scores\": {{\n",
    "        \"atomicity\": 5,\n",
    "        \"bridge_building\": 5,\n",
    "        \"efficiency\": 5,\n",
    "        \"correctness\": 5,\n",
    "        \"retrieval_flag\": 5\n",
    "      }},\n",
    "      \"average\": 5.0,\n",
    "      \"feedback\": \"Brief explanation\"\n",
    "    }}\n",
    "  ],\n",
    "  \"overall_average\": 5.0,\n",
    "  \"overall_feedback\": \"Brief summary\"\n",
    "}}\"\"\"\n",
    "\n",
    "    def judge_decomposition(self, original_question: str, decomposed_questions: List[Dict]) -> Dict:\n",
    "        \"\"\"Evaluate a single decomposition.\"\"\"\n",
    "        prompt = self.create_judge_prompt(original_question, decomposed_questions)\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert evaluator of question decomposition quality. Provide detailed, fair evaluations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            result[\"original_question\"] = original_question\n",
    "            result[\"decomposed_questions\"] = decomposed_questions\n",
    "\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"original_question\": original_question,\n",
    "                \"evaluations\": [],\n",
    "                \"overall_average\": 0.0\n",
    "            }\n",
    "\n",
    "    def compute_aggregate_metrics(self, evaluation_results: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Compute aggregate metrics across evaluations.\"\"\"\n",
    "        valid_results = [r for r in evaluation_results if \"error\" not in r]\n",
    "\n",
    "        if not valid_results:\n",
    "            return {\"error_rate\": 1.0}\n",
    "\n",
    "        overall_scores = [r[\"overall_average\"] for r in valid_results]\n",
    "\n",
    "        # Aggregate per-criterion scores\n",
    "        all_scores = {\n",
    "            \"atomicity\": [],\n",
    "            \"bridge_building\": [],\n",
    "            \"efficiency\": [],\n",
    "            \"correctness\": [],\n",
    "            \"retrieval_flag\": []\n",
    "        }\n",
    "\n",
    "        for result in valid_results:\n",
    "            for eval_item in result.get(\"evaluations\", []):\n",
    "                scores = eval_item.get(\"scores\", {})\n",
    "                for criterion in all_scores:\n",
    "                    if criterion in scores:\n",
    "                        all_scores[criterion].append(scores[criterion])\n",
    "\n",
    "        metrics = {\n",
    "            \"overall_average\": sum(overall_scores) / len(overall_scores) if overall_scores else 0,\n",
    "            \"num_evaluated\": len(valid_results),\n",
    "            \"error_rate\": (len(evaluation_results) - len(valid_results)) / len(evaluation_results)\n",
    "        }\n",
    "\n",
    "        for criterion, values in all_scores.items():\n",
    "            if values:\n",
    "                metrics[f\"{criterion}_avg\"] = sum(values) / len(values)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "print(\"‚úì QuestionDecompositionJudge class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZJtJJT1QLjXu"
   },
   "outputs": [],
   "source": [
    "# Evaluation Callback - Runs during evaluation\n",
    "from transformers import TrainerCallback\n",
    "import random\n",
    "\n",
    "class LLMJudgeEvaluationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback that evaluates model outputs using LLM judge during evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_dataset,\n",
    "        judge: QuestionDecompositionJudge,\n",
    "        tokenizer,\n",
    "        chat_template: str,\n",
    "        num_samples: int = 200,\n",
    "        logs_dir: str = \"./judge_logs\"\n",
    "    ):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.judge = judge\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chat_template = chat_template\n",
    "        self.num_samples = num_samples\n",
    "        self.logs_dir = logs_dir\n",
    "\n",
    "        # Create logs directory\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"Called when trainer runs evaluation.\"\"\"\n",
    "        current_step = state.global_step\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç Running LLM Judge Evaluation at step {current_step}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Get the actual training model from kwargs\n",
    "        model = kwargs.get('model')\n",
    "        if model is None:\n",
    "            print(\"‚ö†Ô∏è  Model not found in kwargs, skipping evaluation\")\n",
    "            return\n",
    "\n",
    "        # Sample examples from eval set\n",
    "        eval_indices = random.sample(range(len(self.eval_dataset)), min(self.num_samples, len(self.eval_dataset)))\n",
    "        eval_samples = [self.eval_dataset[i] for i in eval_indices]\n",
    "\n",
    "        # Model is already in eval mode by Trainer, no need to set it\n",
    "        # Generate decompositions with the model\n",
    "        evaluation_results = []\n",
    "\n",
    "        for i, sample in enumerate(eval_samples):\n",
    "            # Get the original question directly from the dataset\n",
    "            # eval_raw has: question_id, original_question, decomposed_questions, question_type\n",
    "            original_question = sample[\"original_question\"]\n",
    "            \n",
    "            # Create the user prompt using the same format as training\n",
    "            user_prompt = f\"\"\"Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
    "\n",
    "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
    "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure out how they connect.\n",
    "\n",
    "---\n",
    "## A Simple Analogy for Efficiency\n",
    "\n",
    "**Question:** \"What is the phone number of the mother of the tallest player on the Lakers?\"\n",
    "\n",
    "** Inefficient Path:**\n",
    "1.  Who are the players on the Lakers?\n",
    "2.  What are all their heights?\n",
    "3.  Who is the mother of the tallest player? *(This step is a logical leap)*\n",
    "\n",
    "** Efficient Path:**\n",
    "1.  Who is the tallest player on the Lakers?\n",
    "2.  Who is the mother of `<ENTITY_Q1>`?\n",
    "3.  What is the phone number of `<ENTITY_Q2>`?\n",
    "\n",
    "---\n",
    "## How to Decompose a Question\n",
    "This process follows a logical flow from high-level analysis to the fine-tuning of your question chain.\n",
    "\n",
    "### 1. Analyze the Query's Components\n",
    "First, break down the original question into its fundamental building blocks. Identify the core **entities** (people, places, organizations), their **properties** (attributes like rank, location, date), and the **relationships** that connect them.\n",
    "\n",
    "### 2. Construct an Atomic Chain\n",
    "Next, formulate a sequence of questions where each question retrieves a single fact.\n",
    "* **Isolate Comparisons:** Don't ask \"who is faster?\" Ask for the specific rank or time of each person involved.\n",
    "* **Link with Placeholders:** Use `<ENTITY_Qn>` to pass the answer from a previous question (`Qn`) into the next one.\n",
    "\n",
    "### 3. Optimize for Efficiency and Precision\n",
    "Your final goal is the **shortest and most direct path** to the answer.\n",
    "* **Embed Constraints to Build Bridges:** If a piece of information is only a filter (like a date or location), embed it as a constraint in the next question instead of asking for it directly.\n",
    "**Important note for bridges:** There can be no `<ENTITY_Qn>` in the first question if the nth question DOES NOT require retrieval.\n",
    "\n",
    "## Formatting\n",
    "Format each decomposed question as follows:\n",
    "\n",
    "Question: [the question text]\n",
    "Requires retrieval: [true/false]\n",
    "\n",
    "And provide the response in the following JSON format:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"question\": \"the decomposed question text\",\n",
    "      \"requires_retrieval\": \"true/false\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Casablanca?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Input: \"Which film has the director who is older, Dune or The Dark Knight?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Dune?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who directed The Dark Knight?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": False\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "IMPORTANT:\n",
    "    AVOID over-decomposition like this:\n",
    "    DON'T break \"Who is John Doe?\" into:\n",
    "    1. Who is John Doe? ‚Üí \"English\"\n",
    "    2. When was <ENTITY_Q1> born? ‚Üí \"When was English born?\"\n",
    "\n",
    "    DO ask directly: \"When was John Doe born?\"\n",
    "\n",
    "Now decompose this question:\n",
    "Input: \"{original_question}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "            # Generate decomposition\n",
    "            messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                chat_template=self.chat_template\n",
    "            )\n",
    "\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Use BF16 autocast to match training environment's dtype handling\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=512,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.9,\n",
    "                        do_sample=True,\n",
    "                        use_cache=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "            # Decode output\n",
    "            generated_text = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "            # Parse JSON from generated text\n",
    "            try:\n",
    "                # Extract JSON from the output\n",
    "                json_start = generated_text.find(\"{\")\n",
    "                json_end = generated_text.rfind(\"}\") + 1\n",
    "                if json_start != -1 and json_end > json_start:\n",
    "                    json_str = generated_text[json_start:json_end]\n",
    "                    generated_decomp = json.loads(json_str)\n",
    "                    decomposed_questions = generated_decomp.get(\"questions\", [])\n",
    "                else:\n",
    "                    decomposed_questions = []\n",
    "            except:\n",
    "                decomposed_questions = []\n",
    "\n",
    "            # Judge the decomposition\n",
    "            if decomposed_questions:\n",
    "                result = self.judge.judge_decomposition(original_question, decomposed_questions)\n",
    "                evaluation_results.append(result)\n",
    "\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Evaluated {i + 1}/{len(eval_samples)} samples...\")\n",
    "                print(f\"  {len(decomposed_questions)} were generated successfully.\")\n",
    "\n",
    "        # Compute aggregate metrics\n",
    "        if evaluation_results:\n",
    "            judge_metrics = self.judge.compute_aggregate_metrics(evaluation_results)\n",
    "\n",
    "            print(f\"\\nüìä Evaluation Results (Step {current_step}):\")\n",
    "            print(f\"  Overall Average: {judge_metrics.get('overall_average', 0):.2f}/5.0\")\n",
    "            print(f\"  Atomicity: {judge_metrics.get('atomicity_avg', 0):.2f}/5.0\")\n",
    "            print(f\"  Bridge Building: {judge_metrics.get('bridge_building_avg', 0):.2f}/5.0\")\n",
    "            print(f\"  Efficiency: {judge_metrics.get('efficiency_avg', 0):.2f}/5.0\")\n",
    "            print(f\"  Correctness: {judge_metrics.get('correctness_avg', 0):.2f}/5.0\")\n",
    "            print(f\"  Retrieval Flag: {judge_metrics.get('retrieval_flag_avg', 0):.2f}/5.0\")\n",
    "            print(f\"  Evaluated: {judge_metrics.get('num_evaluated', 0)} samples\")\n",
    "\n",
    "            # Log to WandB if available\n",
    "            if state.is_world_process_zero:\n",
    "                try:\n",
    "                    import wandb\n",
    "                    # Don't specify step - let WandB auto-increment\n",
    "                    wandb.log({\n",
    "                        \"judge/overall_average\": judge_metrics.get(\"overall_average\", 0),\n",
    "                        \"judge/atomicity\": judge_metrics.get(\"atomicity_avg\", 0),\n",
    "                        \"judge/bridge_building\": judge_metrics.get(\"bridge_building_avg\", 0),\n",
    "                        \"judge/efficiency\": judge_metrics.get(\"efficiency_avg\", 0),\n",
    "                        \"judge/correctness\": judge_metrics.get(\"correctness_avg\", 0),\n",
    "                        \"judge/retrieval_flag\": judge_metrics.get(\"retrieval_flag_avg\", 0),\n",
    "                        \"judge/num_evaluated\": judge_metrics.get(\"num_evaluated\", 0),\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Save detailed results to JSON\n",
    "            log_file = os.path.join(self.logs_dir, f\"judge_eval_step_{current_step}.json\")\n",
    "            with open(log_file, \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"step\": current_step,\n",
    "                    \"metrics\": judge_metrics,\n",
    "                    \"detailed_results\": evaluation_results\n",
    "                }, f, indent=2)\n",
    "\n",
    "            print(f\"  üíæ Detailed results saved to: {log_file}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  No valid evaluations generated at step {current_step}\")\n",
    "\n",
    "        print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kec1LbbxL9jF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=64): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2027/2027 [00:12<00:00, 156.44 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=64): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:13<00:00, 37.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Trainer configured with:\n",
      "  - Training examples: 2027\n",
      "  - Evaluation examples: 500\n",
      "  - Evaluation strategy: every 20 steps\n",
      "  - LLM judge samples per evaluation: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from typing import Union, List, Dict, Any\n",
    "\n",
    "# Load the new chat template\n",
    "new_chat_template = open('/home/yigit/codebase/gsw-memory/playground/question_decomp_local/qwen3_nonthinking.jinja').read()\n",
    "\n",
    "def formatting_function(example):\n",
    "    \"\"\"\n",
    "    Unsloth requires List[str].\n",
    "    - If 'example[\"messages\"]' is a list of message-lists (batched), return one\n",
    "      formatted string per item.\n",
    "    - If it's a single list of messages (single sample), return a 1-element list.\n",
    "    \"\"\"\n",
    "    msgs = example[\"messages\"]\n",
    "\n",
    "    # Batched: msgs is like [ [ {role, content}, ... ], [ {role, content}, ... ], ... ]\n",
    "    if isinstance(msgs, list) and msgs and isinstance(msgs[0], list):\n",
    "        texts = [\n",
    "            tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=False, chat_template=new_chat_template)\n",
    "            for m in msgs\n",
    "        ]\n",
    "    else:\n",
    "        # Single example\n",
    "        texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                msgs, tokenize=False, add_generation_prompt=False, chat_template=new_chat_template\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Ensure flat list[str] with no empties\n",
    "    return [t for t in texts if isinstance(t, str) and t.strip()]\n",
    "\n",
    "\n",
    "# Create SFTConfig (formatting_func goes to SFTTrainer, not here)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./qwen3_0.6b_question_decomp_full_ft\",  # Required parameter\n",
    "    per_device_train_batch_size = 8,  # Can increase for 0.6B model\n",
    "    gradient_accumulation_steps = 8,   # Use GA to mimic batch size!\n",
    "    warmup_steps = 5,\n",
    "    # num_train_epochs = 1,            # Set this for 1 full training run.\n",
    "    max_steps = 200,                    # For quick testing\n",
    "    max_length=2048,\n",
    "    learning_rate = 5e-6,              # Lower LR for full finetuning (was 2e-4 for LoRA)\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,               # Increased weight decay for full finetuning\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    bf16 = True,                       # Use BF16 precision for full finetuning\n",
    "    # fp16 = True,                         # Use FP16 precision for full finetuning on T4\n",
    "    gradient_checkpointing = True,     # Enable to save memory\n",
    "    report_to = \"wandb\",                # Use TrackIO/WandB etc\n",
    "    # Evaluation configuration - runs LLM judge every 3 steps\n",
    "    eval_strategy=\"steps\",             # Evaluate at regular step intervals\n",
    "    eval_steps=20,                      # Run evaluation every 3 training steps\n",
    ")\n",
    "\n",
    "# Initialize LLM Judge\n",
    "judge = QuestionDecompositionJudge(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create evaluation callback (uses on_evaluate, triggered by eval_strategy)\n",
    "# Pass eval_raw instead of eval_dataset to get original_question field\n",
    "judge_callback = LLMJudgeEvaluationCallback(\n",
    "    eval_dataset=eval_raw,              # Use raw dataset with original_question field\n",
    "    judge=judge,\n",
    "    tokenizer=tokenizer,\n",
    "    chat_template=new_chat_template,\n",
    "    num_samples=200,         # Evaluate 20 samples each time\n",
    "    logs_dir=\"./judge_logs\"\n",
    ")\n",
    "\n",
    "# Create trainer with eval dataset and callback\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = training_dataset,  # 600 examples\n",
    "    eval_dataset = eval_dataset,        # 200 examples (used for standard eval + judge)\n",
    "    formatting_func = formatting_function,\n",
    "    args = training_args,\n",
    "    # callbacks = [judge_callback],      # Add LLM judge callback\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer configured with:\")\n",
    "print(f\"  - Training examples: {len(training_dataset)}\")\n",
    "print(f\"  - Evaluation examples: {len(eval_dataset)}\")\n",
    "print(f\"  - Evaluation strategy: every {training_args.eval_steps} steps\")\n",
    "print(f\"  - LLM judge samples per evaluation: {judge_callback.num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model with Full Finetuning\n",
    "\n",
    "Now let's train our model using **full finetuning** (all parameters are updated, not just LoRA adapters).\n",
    "\n",
    "We do 30 steps for quick testing, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
    "\n",
    "**Important notes for full finetuning:**\n",
    "- Learning rate is much lower (5e-6 vs 2e-4 for LoRA)\n",
    "- Training will take longer as all parameters are updated\n",
    "- Memory usage is higher, but gradient checkpointing helps\n",
    "- The model quality can be better than LoRA since all parameters are trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "95_Nn-89DhsL"
   },
   "outputs": [],
   "source": [
    "# from trl import SFTTrainer, SFTConfig\n",
    "# from typing import Union, List, Dict, Any\n",
    "\n",
    "# # Load the new chat template\n",
    "# new_chat_template = open('/home/yigit/codebase/gsw-memory/playground/question_decomp_local/qwen3_nonthinking.jinja').read()\n",
    "\n",
    "# def formatting_function(example):\n",
    "#     \"\"\"\n",
    "#     Unsloth requires List[str].\n",
    "#     - If 'example[\"messages\"]' is a list of message-lists (batched), return one\n",
    "#       formatted string per item.\n",
    "#     - If it's a single list of messages (single sample), return a 1-element list.\n",
    "#     \"\"\"\n",
    "#     msgs = example[\"messages\"]\n",
    "\n",
    "#     # Batched: msgs is like [ [ {role, content}, ... ], [ {role, content}, ... ], ... ]\n",
    "#     if isinstance(msgs, list) and msgs and isinstance(msgs[0], list):\n",
    "#         texts = [\n",
    "#             tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=False, chat_template=new_chat_template)\n",
    "#             for m in msgs\n",
    "#         ]\n",
    "#     else:\n",
    "#         # Single example\n",
    "#         texts = [\n",
    "#             tokenizer.apply_chat_template(\n",
    "#                 msgs, tokenize=False, add_generation_prompt=False, chat_template=new_chat_template\n",
    "#             )\n",
    "#         ]\n",
    "\n",
    "#     # Ensure flat list[str] with no empties\n",
    "#     return [t for t in texts if isinstance(t, str) and t.strip()]\n",
    "\n",
    "\n",
    "# # Create SFTConfig (formatting_func goes to SFTTrainer, not here)\n",
    "# training_args = SFTConfig(\n",
    "#     output_dir=\"./qwen3_0.6b_question_decomp_full_ft\",  # Required parameter\n",
    "#     per_device_train_batch_size = 2,  # Can increase for 0.6B model\n",
    "#     gradient_accumulation_steps = 4,   # Use GA to mimic batch size!\n",
    "#     warmup_steps = 5,\n",
    "#     # num_train_epochs = 1,            # Set this for 1 full training run.\n",
    "#     max_steps = 200,                    # For quick testing\n",
    "#     max_length=2048,\n",
    "#     learning_rate = 5e-6,              # Lower LR for full finetuning (was 2e-4 for LoRA)\n",
    "#     logging_steps = 1,\n",
    "#     optim = \"adamw_8bit\",\n",
    "#     weight_decay = 0.01,               # Increased weight decay for full finetuning\n",
    "#     lr_scheduler_type = \"linear\",\n",
    "#     seed = 3407,\n",
    "#     # bf16 = True,                       # Use BF16 precision for full finetuning\n",
    "#     gradient_checkpointing = True,     # Enable to save memory\n",
    "#     report_to = \"wandb\",                # Use TrackIO/WandB etc\n",
    "# )\n",
    "\n",
    "# # Create trainer - formatting_func is passed to SFTTrainer, not SFTConfig\n",
    "# # Note: We removed the custom data_collator that was causing the \"no loss\" error\n",
    "# # SFTTrainer will use its default DataCollatorForLanguageModeling which properly handles labels\n",
    "# trainer = SFTTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = training_dataset,  # Using the question decomposition dataset\n",
    "#     eval_dataset = None, # Can set up evaluation!\n",
    "#     formatting_func = formatting_function,  # Pass to SFTTrainer in newer trl versions\n",
    "#     args = training_args,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "2ejIt2xSNKKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX A6000. Max memory = 47.402 GB.\n",
      "1.135 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "u59DPlp7NyaR"
   },
   "outputs": [],
   "source": [
    "# clear gpu\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9fa371ShyhB"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yqxqAZ7KJ4oL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,027 | Num Epochs = 7 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 596,049,920 of 596,049,920 (100.00% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyigitturali\u001b[0m (\u001b[33mmyigitturali-UCLA\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yigit/codebase/gsw-memory/playground/question_decomp_local/wandb/run-20251118_132325-p8o6bt0m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/myigitturali-UCLA/huggingface/runs/p8o6bt0m' target=\"_blank\">peach-sea-204</a></strong> to <a href='https://wandb.ai/myigitturali-UCLA/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/myigitturali-UCLA/huggingface' target=\"_blank\">https://wandb.ai/myigitturali-UCLA/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/myigitturali-UCLA/huggingface/runs/p8o6bt0m' target=\"_blank\">https://wandb.ai/myigitturali-UCLA/huggingface/runs/p8o6bt0m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 34:44, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.546800</td>\n",
       "      <td>1.517792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.035800</td>\n",
       "      <td>1.012272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.514100</td>\n",
       "      <td>0.490573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.233057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.168657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.150111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.145187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.143330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.142778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.142741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3Model does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "pCqnaKmlO1U9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2109.1378 seconds used for training.\n",
      "35.15 minutes used for training.\n",
      "Peak reserved memory = 10.734 GB.\n",
      "Peak reserved memory for training = 9.599 GB.\n",
      "Peak reserved memory % of max memory = 22.645 %.\n",
      "Peak reserved memory for training % of max memory = 20.25 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference - Question Decomposition\n",
    "\n",
    "Let's test the model on question decomposition! The model should break down complex multi-hop questions into atomic single-hop questions.\n",
    "\n",
    "The model will output JSON format with:\n",
    "- `questions`: List of decomposed sub-questions\n",
    "- `requires_retrieval`: Boolean flag for each question indicating if it needs retrieval\n",
    "\n",
    "For structured JSON output, use lower temperature (0.0-0.3) for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kR3gIAX-SM2q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where did Lothair II's mother die?\n",
      "\n",
      "Decomposition:\n",
      "{\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"question\": \"Who was Lothair II's mother?\",\n",
      "            \"requires_retrieval\": true\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Where did <ENTITY_Q1> die?\",\n",
      "            \"requires_retrieval\": true\n",
      "        }\n",
      "    ]\n",
      "}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Load the new chat template for inference\n",
    "new_chat_template = open('/home/yigit/codebase/gsw-memory/playground/question_decomp_local/qwen3_nonthinking.jinja').read()\n",
    "\n",
    "# Example 1: Test question decomposition with a 2-hop question\n",
    "test_question = \"Where did Lothair II's mother die?\"\n",
    "\n",
    "# Create the prompt using the same format as training\n",
    "user_prompt = f\"\"\"Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
    "\n",
    "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
    "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure out how they connect.\n",
    "\n",
    "---\n",
    "## A Simple Analogy for Efficiency\n",
    "\n",
    "**Question:** \"What is the phone number of the mother of the tallest player on the Lakers?\"\n",
    "\n",
    "** Inefficient Path:**\n",
    "1.  Who are the players on the Lakers?\n",
    "2.  What are all their heights?\n",
    "3.  Who is the mother of the tallest player? *(This step is a logical leap)*\n",
    "\n",
    "** Efficient Path:**\n",
    "1.  Who is the tallest player on the Lakers?\n",
    "2.  Who is the mother of `<ENTITY_Q1>`?\n",
    "3.  What is the phone number of `<ENTITY_Q2>`?\n",
    "\n",
    "---\n",
    "## How to Decompose a Question\n",
    "This process follows a logical flow from high-level analysis to the fine-tuning of your question chain.\n",
    "\n",
    "### 1. Analyze the Query's Components\n",
    "First, break down the original question into its fundamental building blocks. Identify the core **entities** (people, places, organizations), their **properties** (attributes like rank, location, date), and the **relationships** that connect them.\n",
    "\n",
    "### 2. Construct an Atomic Chain\n",
    "Next, formulate a sequence of questions where each question retrieves a single fact.\n",
    "* **Isolate Comparisons:** Don't ask \"who is faster?\" Ask for the specific rank or time of each person involved.\n",
    "* **Link with Placeholders:** Use `<ENTITY_Qn>` to pass the answer from a previous question (`Qn`) into the next one.\n",
    "\n",
    "### 3. Optimize for Efficiency and Precision\n",
    "Your final goal is the **shortest and most direct path** to the answer.\n",
    "* **Embed Constraints to Build Bridges:** If a piece of information is only a filter (like a date or location), embed it as a constraint in the next question instead of asking for it directly.\n",
    "**Important note for bridges:** There can be no `<ENTITY_Qn>` in the first question if the nth question DOES NOT require retrieval.\n",
    "\n",
    "## Formatting\n",
    "Format each decomposed question as follows:\n",
    "\n",
    "Question: [the question text]\n",
    "Requires retrieval: [true/false]\n",
    "\n",
    "And provide the response in the following JSON format:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"question\": \"the decomposed question text\",\n",
    "      \"requires_retrieval\": \"True/False\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Casablanca?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Input: \"Which film has the director who is older, Dune or The Dark Knight?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Dune?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who directed The Dark Knight?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": False\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "IMPORTANT:\n",
    "    AVOID over-decomposition like this:\n",
    "    DON'T break \"Who is John Doe?\" into:\n",
    "    1. Who is John Doe? ‚Üí \"English\"\n",
    "    2. When was <ENTITY_Q1> born? ‚Üí \"When was English born?\"\n",
    "\n",
    "    DO ask directly: \"When was John Doe born?\"\n",
    "\n",
    "Now decompose this question:\n",
    "Input: \"{test_question}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    chat_template=new_chat_template # Pass the new template here\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare inputs and cast to float16 for consistency during inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(\"Decomposition:\")\n",
    "\n",
    "# Ensure model is in evaluation mode and cast to float16\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Explicitly disable autocast, as we are forcing float16 already.\n",
    "    # This prevents any potential re-casting issues if autocast implicitly tries to use float32 for some ops.\n",
    "    with torch.amp.autocast('cuda', enabled=False): # Updated syntax\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,  # Enough for JSON output\n",
    "            temperature=0.1,     # Low temperature for consistent JSON\n",
    "            top_p=0.9,\n",
    "            streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j873RMcEi9uq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the birth year of the spouse of the director of Casablanca?\n",
      "\n",
      "Decomposition:\n",
      "{\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"question\": \"Who directed Casablanca?\",\n",
      "            \"requires_retrieval\": true\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
      "            \"requires_retrieval\": true\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
      "            \"requires_retrieval\": true\n",
      "        }\n",
      "    ]\n",
      "}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Load the new chat template for inference\n",
    "new_chat_template = open('/home/yigit/codebase/gsw-memory/playground/question_decomp_local/qwen3_nonthinking.jinja').read()\n",
    "\n",
    "# Example 2: Test with a more complex 3-hop question\n",
    "test_question_2 = \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "\n",
    "user_prompt_2 = f\"\"\"Your task is to break down a complex multi-hop question into the most efficient sequence of single-hop, **atomic** questions.\n",
    "\n",
    "## Your Main Goal: Build Smart Bridges, Don't Just Collect Nouns\n",
    "The most critical skill is to convert complex logical clauses (like \"despite,\" \"the country where,\" \"the year before\") into a single, powerful **bridging question**. This question should use a known entity as context to find the next one. Avoid finding all the entities separately and then trying to figure out how they connect.\n",
    "\n",
    "---\n",
    "## A Simple Analogy for Efficiency\n",
    "\n",
    "**Question:** \"What is the phone number of the mother of the tallest player on the Lakers?\"\n",
    "\n",
    "** Inefficient Path:**\n",
    "1.  Who are the players on the Lakers?\n",
    "2.  What are all their heights?\n",
    "3.  Who is the mother of the tallest player? *(This step is a logical leap)*\n",
    "\n",
    "** Efficient Path:**\n",
    "1.  Who is the tallest player on the Lakers?\n",
    "2.  Who is the mother of `<ENTITY_Q1>`?\n",
    "3.  What is the phone number of `<ENTITY_Q2>`?\n",
    "\n",
    "---\n",
    "## How to Decompose a Question\n",
    "This process follows a logical flow from high-level analysis to the fine-tuning of your question chain.\n",
    "\n",
    "### 1. Analyze the Query's Components\n",
    "First, break down the original question into its fundamental building blocks. Identify the core **entities** (people, places, organizations), their **properties** (attributes like rank, location, date), and the **relationships** that connect them.\n",
    "\n",
    "### 2. Construct an Atomic Chain\n",
    "Next, formulate a sequence of questions where each question retrieves a single fact.\n",
    "* **Isolate Comparisons:** Don't ask \"who is faster?\" Ask for the specific rank or time of each person involved.\n",
    "* **Link with Placeholders:** Use `<ENTITY_Qn>` to pass the answer from a previous question (`Qn`) into the next one.\n",
    "\n",
    "### 3. Optimize for Efficiency and Precision\n",
    "Your final goal is the **shortest and most direct path** to the answer.\n",
    "* **Embed Constraints to Build Bridges:** If a piece of information is only a filter (like a date or location), embed it as a constraint in the next question instead of asking for it directly.\n",
    "**Important note for bridges:** There can be no `<ENTITY_Qn>` in the first question if the nth question DOES NOT require retrieval.\n",
    "\n",
    "## Formatting\n",
    "Format each decomposed question as follows:\n",
    "\n",
    "Question: [the question text]\n",
    "Requires retrieval: [true/false]\n",
    "\n",
    "And provide the response in the following JSON format:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"question\": \"the decomposed question text\",\n",
    "      \"requires_retrieval\": \"true/false\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"What is the birth year of the spouse of the director of Casablanca?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Casablanca?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who was <ENTITY_Q1>'s spouse?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"What is <ENTITY_Q2>'s birth year?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Input: \"Which film has the director who is older, Dune or The Dark Knight?\"\n",
    "Output:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Who directed Dune?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who directed The Dark Knight?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": True\n",
    "        }},\n",
    "        {{\n",
    "            \"question\": \"Who is older, <ENTITY_Q1> or <ENTITY_Q2>?\",\n",
    "            \"requires_retrieval\": False\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "\n",
    "IMPORTANT:\n",
    "    AVOID over-decomposition like this:\n",
    "    DON'T break \"Who is John Doe?\" into:\n",
    "    1. Who is John Doe? ‚Üí \"English\"\n",
    "    2. When was <ENTITY_Q1> born? ‚Üí \"When was English born?\"\n",
    "\n",
    "    DO ask directly: \"When was John Doe born?\"\n",
    "\n",
    "Now decompose this question:\n",
    "Input: \"{test_question_2}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "messages_2 = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt_2}\n",
    "]\n",
    "\n",
    "text_2 = tokenizer.apply_chat_template(\n",
    "    messages_2,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    chat_template=new_chat_template # Pass the new template here\n",
    ")\n",
    "\n",
    "# Prepare inputs and cast to float16 for consistency during inference\n",
    "inputs_2 = tokenizer(text_2, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"\\nQuestion: {test_question_2}\\n\")\n",
    "print(\"Decomposition:\")\n",
    "\n",
    "# Ensure model is in evaluation mode and cast to float16\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Explicitly disable autocast, as we are forcing float16 already.\n",
    "    # This prevents any potential re-casting issues if autocast implicitly tries to use float32 for some ops.\n",
    "    with torch.amp.autocast('cuda', enabled=False): # Updated syntax\n",
    "        _ = model.generate(\n",
    "            **inputs_2,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "### Saving the fully finetuned model\n",
    "For full finetuning, we save the **entire model** (not just adapters like LoRA). You can use Huggingface's `push_to_hub` for online save or `save_pretrained` for local save.\n",
    "\n",
    "**[NOTE]** This saves the complete model with all trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "upcOlWe7A1vc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.19GB / 1.19GB, 83.8MB/s  \n",
      "New Data Upload: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.19GB / 1.19GB, 83.8MB/s  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/yigitturali/qwen3-0.6b-gsw-q-decomp-finetuned-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.4MB / 11.4MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "[huggingface_hub.hf_api|WARNING]No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "model.push_to_hub(\"yigitturali/qwen3-0.6b-gsw-q-decomp-finetuned-large\")\n",
    "tokenizer.push_to_hub(\"yigitturali/qwen3-0.6b-gsw-q-decomp-finetuned-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the fully finetuned model we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"full_finetuned_model\", # Path to your saved model\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,  # Can use 4bit for inference to save memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "For full finetuning, you can save the model in different formats. Select `merged_16bit` for float16 or `merged_4bit` for int4. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.\n",
    "\n",
    "**Note:** For full finetuning, the model is already \"merged\" (no adapters to merge), so these methods will save the complete model in the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Save full model to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Save full model to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Save full model in standard format\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "For full finetuning, the entire model will be converted to GGUF format.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "model_trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
