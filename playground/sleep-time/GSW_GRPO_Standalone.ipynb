{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSW Sleep-Time Agent â€” Standalone Multi-Turn GRPO\n",
    "\n",
    "Train Qwen3-30B-A3B (MoE) to explore GSW structures and create bridge QA pairs.\n",
    "\n",
    "**No ART framework** â€” uses Unsloth + manual GRPO directly.\n",
    "\n",
    "- Unsloth loads model with `fast_inference=False` (required for MoE)\n",
    "- Multi-turn rollouts via `model.generate()` + local GSW tool execution\n",
    "- Manual GRPO: advantage-weighted policy gradient with tool-response masking\n",
    "- Reward: bridge-F1 against MuSiQue gold answers (verifiable, no LLM judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-30B-A3B\n",
      "LoRA rank: 32\n",
      "GRPO group size: 4\n",
      "Training GPUs: 0, 1 | Embedding GPU: 2\n",
      "HF_HOME: /mnt/SSD3/yigit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/SSD3/yigit\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3\"\n",
    "# ---- Model ----\n",
    "BASE_MODEL = \"Qwen/Qwen3-30B-A3B\"\n",
    "MAX_SEQ_LENGTH = 32768\n",
    "LORA_RANK = 32\n",
    "\n",
    "# ---- GPU ----\n",
    "EMBEDDING_GPU = 2  # GPU for EntitySearcher's embedding model (GPUs 0+1 used by training model)\n",
    "\n",
    "# ---- Rollout ----\n",
    "MAX_TURNS = 30\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 32768  # per generation step\n",
    "\n",
    "# ---- GRPO ----\n",
    "ROLLOUTS_PER_PROMPT = 4   # group size\n",
    "NUM_TRAIN_PROMPTS = 2     # prompts per step\n",
    "KL_COEF = 0.001\n",
    "CLIP_RATIO = 0.2\n",
    "\n",
    "# ---- Training ----\n",
    "MAX_STEPS = 200\n",
    "LEARNING_RATE = 5e-6\n",
    "MAX_GRAD_NORM = 1.0\n",
    "SAVE_EVERY = 50\n",
    "LOG_EVERY = 5\n",
    "\n",
    "# ---- Data ----\n",
    "INDEX_PATH = \"/home/yigit/codebase/gsw-memory/data/rl_training/index.json\"\n",
    "VAL_SPLIT = 0.05\n",
    "OUTPUT_DIR = \"outputs/gsw_grpo\"\n",
    "\n",
    "print(f\"Model: {BASE_MODEL}\")\n",
    "print(f\"LoRA rank: {LORA_RANK}\")\n",
    "print(f\"GRPO group size: {ROLLOUTS_PER_PROMPT}\")\n",
    "print(f\"Training GPUs: 0, 1 | Embedding GPU: {EMBEDDING_GPU}\")\n",
    "print(f\"HF_HOME: {os.environ['HF_HOME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1+cu126\n",
      "CUDA: 12.6\n",
      "GPUs: 2\n",
      "  GPU 0: NVIDIA RTX A6000 | 47.4 GB total | 47.1 GB free\n",
      "  GPU 1: NVIDIA RTX A6000 | 47.4 GB total | 47.1 GB free\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    free, _ = torch.cuda.mem_get_info(i)\n",
    "    print(f\"  GPU {i}: {name} | {total:.1f} GB total | {free/1024**3:.1f} GB free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model (Unsloth MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 01:01:46 [__init__.py:235] Automatically detected platform cuda.\n",
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "/home/yigit/codebase/gsw-memory/.venv/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEab\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.3: Fast Qwen3_Moe patching. Transformers: 4.56.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 2. Max memory: 47.402 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a217d0876044349066399d45a2932a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "trainable params: 1,687,683,072 || all params: 32,219,805,696 || trainable%: 5.2380\n",
      "  GPU 0: 31.6 / 47.4 GB allocated\n",
      "  GPU 1: 31.6 / 47.4 GB allocated\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=False,  # Not supported for MoE\n",
    "    device_map=\"balanced\",\n",
    "    max_memory={0: \"46GiB\", 1: \"46GiB\"},  # Only use GPU 0 and 1\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\", \"gate_up_proj\",\n",
    "    ],\n",
    "    lora_alpha=LORA_RANK * 2,\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=3407,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Show per-GPU memory usage\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    alloc = torch.cuda.memory_allocated(i) / 1024**3\n",
    "    total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    print(f\"  GPU {i}: {alloc:.1f} / {total:.1f} GB allocated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 500, Train: 475, Val: 25\n",
      "Sample: What month did the person who retained trust in Longchamp go away to the Holy Land?\n",
      "Answer: October\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(INDEX_PATH) as f:\n",
    "    all_scenarios = json.load(f)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(all_scenarios)\n",
    "\n",
    "n_val = max(1, int(len(all_scenarios) * VAL_SPLIT))\n",
    "val_scenarios = all_scenarios[:n_val]\n",
    "train_scenarios = all_scenarios[n_val:]\n",
    "\n",
    "print(f\"Total: {len(all_scenarios)}, Train: {len(train_scenarios)}, Val: {len(val_scenarios)}\")\n",
    "print(f\"Sample: {train_scenarios[0]['question']}\")\n",
    "print(f\"Answer: {train_scenarios[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader: Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader: Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "INFO:faiss.loader: Loading faiss.\n",
      "INFO:faiss.loader: Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout function defined.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "from gsw_memory.sleep_time.environment import GSWEnvironment\n",
    "from gsw_memory.sleep_time.entity_search import EntitySearcher\n",
    "from gsw_memory.sleep_time.prompts import SLEEP_TIME_SYSTEM_PROMPT\n",
    "from gsw_memory.sleep_time.reward import compute_reward\n",
    "\n",
    "# Tool call regex (same as gsw_interaction.py)\n",
    "_TOOL_CALL_RE = re.compile(r\"<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>\", re.DOTALL)\n",
    "\n",
    "\n",
    "def build_messages(question: str) -> list[dict]:\n",
    "    \"\"\"Build initial chat messages for an episode.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SLEEP_TIME_SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Question: {question}\\n\\n\"\n",
    "                f\"Explore the GSW corpus to find multi-hop bridge connections \"\n",
    "                f\"that help answer this question. Use the available tools systematically.\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "def messages_to_text(messages: list[dict]) -> str:\n",
    "    \"\"\"Convert messages to Qwen chat format string.\"\"\"\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_tool_call(text: str) -> tuple[str, dict] | None:\n",
    "    \"\"\"Extract last tool call from generated text.\"\"\"\n",
    "    matches = list(_TOOL_CALL_RE.finditer(text))\n",
    "    if not matches:\n",
    "        return None\n",
    "    try:\n",
    "        call = json.loads(matches[-1].group(1))\n",
    "        name = call.get(\"name\", \"\")\n",
    "        args = call.get(\"arguments\", {})\n",
    "        if isinstance(args, str):\n",
    "            args = json.loads(args)\n",
    "        return name, args\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_episode(\n",
    "    scenario: dict,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_turns: int = MAX_TURNS,\n",
    "    temperature: float = TEMPERATURE,\n",
    "    max_new_tokens: int = MAX_NEW_TOKENS,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run one multi-turn episode.\n",
    "\n",
    "    Returns dict with:\n",
    "        messages: full conversation (list of dicts)\n",
    "        full_text: tokenized trajectory as string\n",
    "        reward: scalar reward\n",
    "        num_bridges: count of bridges created\n",
    "        num_turns: turns used\n",
    "    \"\"\"\n",
    "    # Build environment\n",
    "    gsw_dirs = scenario[\"gsw_dirs\"]\n",
    "    gsw_root = str(Path(gsw_dirs[0]).parent)\n",
    "    searcher = EntitySearcher(\n",
    "        path_to_gsw_files=gsw_root,\n",
    "        verbose=False,\n",
    "        gpu_device=EMBEDDING_GPU,\n",
    "    )\n",
    "    env = GSWEnvironment(\n",
    "        entity_searcher=searcher,\n",
    "        question=scenario[\"question\"],\n",
    "        gold_answer=scenario[\"answer\"],\n",
    "        gold_decomposition=scenario.get(\"decomposition\", []),\n",
    "        max_turns=max_turns,\n",
    "    )\n",
    "    env.reset()\n",
    "\n",
    "    messages = build_messages(scenario[\"question\"])\n",
    "\n",
    "    for turn in range(max_turns):\n",
    "        # Generate\n",
    "        prompt_text = messages_to_text(messages)\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        # Decode only the new tokens\n",
    "        new_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Append assistant message\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "        # Check for tool call\n",
    "        parsed = parse_tool_call(response_text)\n",
    "        if parsed is None:\n",
    "            # No tool call â€” agent stopped\n",
    "            break\n",
    "\n",
    "        tool_name, tool_args = parsed\n",
    "\n",
    "        # Execute tool\n",
    "        try:\n",
    "            obs, done = env.step(tool_name, tool_args)\n",
    "        except Exception as e:\n",
    "            obs = json.dumps({\"error\": str(e)})\n",
    "            done = False\n",
    "\n",
    "        # Append tool result as user message (for next generation)\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"<tool_response>{obs}</tool_response>\",\n",
    "        })\n",
    "\n",
    "        if done or env.done:\n",
    "            break\n",
    "\n",
    "    # Compute reward\n",
    "    bridges = env.get_bridges()\n",
    "    reward = compute_reward(\n",
    "        bridges=bridges,\n",
    "        gold_answer=scenario[\"answer\"],\n",
    "        gold_decomposition=scenario.get(\"decomposition\", []),\n",
    "        gold_aliases=scenario.get(\"answer_aliases\", []),\n",
    "    )\n",
    "\n",
    "    # Build full text for tokenization\n",
    "    full_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"full_text\": full_text,\n",
    "        \"reward\": reward,\n",
    "        \"num_bridges\": len(bridges),\n",
    "        \"num_turns\": env.turn,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Rollout function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What month did the person who retained trust in Longchamp go away to the Holy Land?\n",
      "Answer: October\n",
      "------------------------------------------------------------\n",
      "Loading first 50 GSW files...\n",
      "Loaded 50 GSW structures from 50 documents\n",
      "INFO 02-25 18:08:05 [config.py:538] Found sentence-transformers modules configuration.\n",
      "INFO 02-25 18:08:05 [config.py:558] Found pooling configuration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 18:08:05 [config.py:1604] Using max model len 40960\n",
      "INFO 02-25 18:08:05 [arg_utils.py:1551] (Enabling) chunked prefill by default\n",
      "INFO 02-25 18:08:05 [arg_utils.py:1554] (Enabling) prefix caching by default\n",
      "INFO 02-25 18:08:05 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 02-25 18:08:06 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 18:08:11 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 02-25 18:08:11 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 02-25 18:08:11 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen3-Embedding-8B', speculative_config=None, tokenizer='Qwen/Qwen3-Embedding-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-Embedding-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 02-25 18:08:13 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 02-25 18:08:13 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 02-25 18:08:13 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen3-Embedding-8B...\n",
      "INFO 02-25 18:08:13 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 02-25 18:08:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 02-25 18:08:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.71it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 18:08:17 [default_loader.py:262] Loading weights took 3.28 seconds\n",
      "INFO 02-25 18:08:18 [gpu_model_runner.py:1892] Model loading took 14.1355 GiB and 4.022104 seconds\n",
      "INFO 02-25 18:08:24 [backends.py:530] Using cache directory: /home/yigit/.cache/vllm/torch_compile_cache/4f805632d6/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 02-25 18:08:24 [backends.py:541] Dynamo bytecode transform time: 5.90 s\n",
      "INFO 02-25 18:08:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.410 s\n",
      "INFO 02-25 18:08:30 [monitor.py:34] torch.compile takes 5.90 s in total\n",
      "INFO 02-25 18:08:31 [gpu_worker.py:255] Available KV cache memory: 27.69 GiB\n",
      "INFO 02-25 18:08:32 [kv_cache_utils.py:833] GPU KV cache size: 201,616 tokens\n",
      "INFO 02-25 18:08:32 [kv_cache_utils.py:837] Maximum concurrency for 40,960 tokens per request: 4.92x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:03<00:00, 16.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 18:08:36 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 0.58 GiB\n",
      "INFO 02-25 18:08:36 [core.py:193] init engine (profile, create kv cache, warmup model) took 18.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W225 18:11:03.134022133 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reward: 0.0000\n",
      "Bridges: 0\n",
      "Turns: 0\n",
      "Messages: 3\n",
      "\n",
      "[0] SYSTEM: (30464 chars)\n",
      "\n",
      "[1] USER: Question: What month did the person who retained trust in Longchamp go away to the Holy Land?\n",
      "\n",
      "Explore the GSW corpus to find multi-hop bridge connections that help answer this question. Use the avail\n",
      "\n",
      "[2] ASSISTANT: <think>\n",
      "Okay, let's tackle this question: \"What month did the person who retained trust in Longchamp go away to the Holy Land?\" \n",
      "\n",
      "First, I need to break down the question. The key entities here are \"Longchamp\" and the person who retained trust in Longchamp. The goal is to find out the month when thi\n"
     ]
    }
   ],
   "source": [
    "test_scenario = train_scenarios[0]\n",
    "print(f\"Question: {test_scenario['question']}\")\n",
    "print(f\"Answer: {test_scenario['answer']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable fast inference mode\n",
    "\n",
    "result = run_episode(test_scenario, model, tokenizer)\n",
    "\n",
    "print(f\"\\nReward: {result['reward']:.4f}\")\n",
    "print(f\"Bridges: {result['num_bridges']}\")\n",
    "print(f\"Turns: {result['num_turns']}\")\n",
    "print(f\"Messages: {len(result['messages'])}\")\n",
    "\n",
    "# Show conversation\n",
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    role = msg[\"role\"]\n",
    "    content = msg[\"content\"]\n",
    "    if role == \"system\":\n",
    "        print(f\"\\n[{i}] SYSTEM: ({len(content)} chars)\")\n",
    "    elif role == \"user\" and \"<tool_response>\" in content:\n",
    "        print(f\"\\n[{i}] TOOL_RESPONSE: {content}...\")\n",
    "    elif role == \"user\":\n",
    "        print(f\"\\n[{i}] USER: {content}\")\n",
    "    elif role == \"assistant\":\n",
    "        print(f\"\\n[{i}] ASSISTANT: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0] SYSTEM: (30464 chars)\n",
      "\n",
      "[1] USER: Question: What month did the person who retained trust in Longchamp go away to the Holy Land?\n",
      "\n",
      "Explore the GSW corpus to find multi-hop bridge connections that help answer this question. Use the available tools systematically.\n",
      "\n",
      "[2] ASSISTANT: <think>\n",
      "Okay, let's tackle this question: \"What month did the person who retained trust in Longchamp go away to the Holy Land?\" \n",
      "\n",
      "First, I need to break down the question. The key entities here are \"Longchamp\" and the person who retained trust in Longchamp. The goal is to find out the month when this person left for the Holy Land. \n",
      "\n",
      "I should start by exploring the entity \"Longchamp\" to see what information is available. Using the tools provided, I'll call reconcile_entity_across_docs(\"Longchamp\") to get a merged view of all documents mentioning Longchamp. This will help me understand the context and any relationships associated with Longchamp.\n",
      "\n",
      "Once I have the merged data, I'll look for any entities that are related to Longchamp, especially those that have a relationship like \"retained trust\" or similar. The next step would be to check the documents where these entities appear. For each related entity, I'll use get_entity_documents to see which documents they are in and then get_entity_context to gather detailed information from those documents.\n",
      "\n",
      "If I find an entity that is connected to Longchamp through trust, I need to explore that entity further. For example, if the entity is \"Person X,\" I'll check where Person X is mentioned and what actions they took, particularly any mentions of traveling to the Holy Land. \n",
      "\n",
      "The challenge here is that the question involves a multi-hop connection: trust in Longchamp â†’ person â†’ departure to Holy Land. So, I need to ensure that I'm following the relationships correctly. If the person who retained trust in Longchamp is mentioned in multiple documents, I'll need to cross-reference those documents to find the month of their departure.\n",
      "\n",
      "I should also be cautious about the tools' usage. After getting the documents for a related entity, I'll use get_entity_context with all relevant documents at once to gather all necessary information efficiently. Then, create bridge QA pairs once I have the necessary data from different documents.\n",
      "\n",
      "If I encounter an entity that only appears in one document, I shouldn't skip it. It might still be part of a bridge between other documents. For example, if the person who retained trust is only in one document, but that document mentions their departure, and another document about Longchamp mentions their trust, that's a valid bridge.\n",
      "\n",
      "Throughout the process, I'll use the tracking tools like plan_entity_exploration, mark_relationship_explored, and get_exploration_status to\n"
     ]
    }
   ],
   "source": [
    "# Show conversation\n",
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    role = msg[\"role\"]\n",
    "    content = msg[\"content\"]\n",
    "    if role == \"system\":\n",
    "        print(f\"\\n[{i}] SYSTEM: ({len(content)} chars)\")\n",
    "    elif role == \"user\" and \"<tool_response>\" in content:\n",
    "        print(f\"\\n[{i}] TOOL_RESPONSE: {content}...\")\n",
    "    elif role == \"user\":\n",
    "        print(f\"\\n[{i}] USER: {content}\")\n",
    "    elif role == \"assistant\":\n",
    "        print(f\"\\n[{i}] ASSISTANT: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Training Loop\n",
    "\n",
    "Manual GRPO: collect rollout groups, compute advantages, do policy gradient updates\n",
    "with tool-response token masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsw_memory.sleep_time.train import compute_advantages, mask_tool_response_tokens\n",
    "from torch.optim import AdamW\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Setup output dir\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Reference model log-probs (for KL penalty) â€” use initial model\n",
    "# We compute ref log-probs inline during the first forward pass\n",
    "# For simplicity, we skip KL penalty initially and add it later if needed\n",
    "\n",
    "\n",
    "def compute_grpo_loss(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    episodes: list[dict],\n",
    "    advantages: list[float],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute GRPO policy gradient loss for a group of episodes.\n",
    "\n",
    "    Loss = -mean(advantage_i * log_prob_i * loss_mask_i)\n",
    "\n",
    "    Tool response tokens are masked from the loss.\n",
    "    \"\"\"\n",
    "    total_loss = torch.tensor(0.0, device=model.device)\n",
    "    n_valid = 0\n",
    "\n",
    "    for ep, adv in zip(episodes, advantages):\n",
    "        # Tokenize full trajectory\n",
    "        full_text = ep[\"full_text\"]\n",
    "        encoding = tokenizer(\n",
    "            full_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "        ).to(model.device)\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"][0]  # (seq_len,)\n",
    "\n",
    "        # Build loss mask (zeros out tool response tokens)\n",
    "        loss_mask = mask_tool_response_tokens(input_ids, tokenizer).to(model.device)\n",
    "\n",
    "        # Find where the first assistant response starts (skip prompt tokens)\n",
    "        # We only train on assistant tokens, not prompt tokens\n",
    "        prompt_messages = ep[\"messages\"][:2]  # system + user\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        prompt_ids = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
    "        prompt_len = len(prompt_ids)\n",
    "\n",
    "        # Zero out prompt tokens in loss mask\n",
    "        loss_mask[:prompt_len] = 0.0\n",
    "\n",
    "        if loss_mask.sum() == 0:\n",
    "            continue  # Nothing to train on\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"],\n",
    "        )\n",
    "        logits = outputs.logits  # (1, seq_len, vocab)\n",
    "\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[0, :-1, :]  # (seq_len-1, vocab)\n",
    "        shift_labels = input_ids[1:]       # (seq_len-1,)\n",
    "        shift_mask = loss_mask[1:]          # (seq_len-1,)\n",
    "\n",
    "        # Log probabilities of actual tokens\n",
    "        log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(1, shift_labels.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Masked mean log prob, weighted by advantage\n",
    "        masked_log_probs = (token_log_probs * shift_mask).sum() / shift_mask.sum().clamp(min=1)\n",
    "        total_loss += -adv * masked_log_probs\n",
    "        n_valid += 1\n",
    "\n",
    "    if n_valid == 0:\n",
    "        return total_loss\n",
    "\n",
    "    return total_loss / n_valid\n",
    "\n",
    "\n",
    "print(\"GRPO loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "all_rewards = []\n",
    "scenario_idx = 0\n",
    "\n",
    "print(f\"Starting GRPO training for {MAX_STEPS} steps...\")\n",
    "print(f\"  {NUM_TRAIN_PROMPTS} prompts x {ROLLOUTS_PER_PROMPT} rollouts = {NUM_TRAIN_PROMPTS * ROLLOUTS_PER_PROMPT} episodes/step\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in range(1, MAX_STEPS + 1):\n",
    "    step_start = time.time()\n",
    "\n",
    "    # ---- Collect rollouts ----\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    step_episodes = []   # list of lists: [prompt_group_1, prompt_group_2, ...]\n",
    "    step_rewards = []\n",
    "\n",
    "    for _ in range(NUM_TRAIN_PROMPTS):\n",
    "        scenario = train_scenarios[scenario_idx % len(train_scenarios)]\n",
    "        scenario_idx += 1\n",
    "\n",
    "        group_episodes = []\n",
    "        group_rewards = []\n",
    "\n",
    "        for _ in range(ROLLOUTS_PER_PROMPT):\n",
    "            try:\n",
    "                ep = run_episode(scenario, model, tokenizer)\n",
    "                group_episodes.append(ep)\n",
    "                group_rewards.append(ep[\"reward\"])\n",
    "            except Exception as e:\n",
    "                print(f\"  Episode error: {e}\")\n",
    "                continue\n",
    "\n",
    "        if group_episodes:\n",
    "            # Compute GRPO advantages within this group\n",
    "            advantages = compute_advantages(group_rewards)\n",
    "            step_episodes.append((group_episodes, advantages))\n",
    "            step_rewards.extend(group_rewards)\n",
    "\n",
    "    if not step_episodes:\n",
    "        print(f\"Step {step}: No valid episodes, skipping.\")\n",
    "        continue\n",
    "\n",
    "    all_rewards.extend(step_rewards)\n",
    "\n",
    "    # ---- GRPO update ----\n",
    "    FastLanguageModel.for_training(model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss = torch.tensor(0.0, device=model.device)\n",
    "    n_groups = 0\n",
    "\n",
    "    for group_episodes, advantages in step_episodes:\n",
    "        loss = compute_grpo_loss(model, tokenizer, group_episodes, advantages)\n",
    "        total_loss = total_loss + loss\n",
    "        n_groups += 1\n",
    "\n",
    "    if n_groups > 0:\n",
    "        avg_loss = total_loss / n_groups\n",
    "        avg_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "\n",
    "    # ---- Logging ----\n",
    "    if step % LOG_EVERY == 0 or step == 1:\n",
    "        recent = all_rewards[-50:] if all_rewards else [0]\n",
    "        mean_r = sum(recent) / len(recent)\n",
    "        max_r = max(recent)\n",
    "        nonzero = sum(1 for r in recent if r > 0) / len(recent)\n",
    "        loss_val = avg_loss.item() if n_groups > 0 else 0.0\n",
    "        print(\n",
    "            f\"Step {step}/{MAX_STEPS} | \"\n",
    "            f\"loss={loss_val:.4f} | \"\n",
    "            f\"reward={mean_r:.3f} (max={max_r:.3f}, nonzero={nonzero:.0%}) | \"\n",
    "            f\"{step_time:.1f}s\"\n",
    "        )\n",
    "\n",
    "    # ---- Checkpoint ----\n",
    "    if step % SAVE_EVERY == 0:\n",
    "        ckpt_dir = output_path / f\"checkpoint_step_{step}\"\n",
    "        ckpt_dir.mkdir(exist_ok=True)\n",
    "        model.save_pretrained(str(ckpt_dir))\n",
    "        tokenizer.save_pretrained(str(ckpt_dir))\n",
    "        print(f\"  Saved checkpoint: {ckpt_dir}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# Save final\n",
    "final_dir = output_path / \"final\"\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "model.save_pretrained(str(final_dir))\n",
    "tokenizer.save_pretrained(str(final_dir))\n",
    "print(f\"Final model saved to: {final_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "val_rewards = []\n",
    "val_bridges = []\n",
    "\n",
    "print(f\"Running validation on {len(val_scenarios)} scenarios...\")\n",
    "for i, scenario in enumerate(val_scenarios):\n",
    "    try:\n",
    "        result = run_episode(scenario, model, tokenizer)\n",
    "        val_rewards.append(result[\"reward\"])\n",
    "        val_bridges.append(result[\"num_bridges\"])\n",
    "        print(f\"  [{i+1}/{len(val_scenarios)}] reward={result['reward']:.3f} bridges={result['num_bridges']} turns={result['num_turns']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [{i+1}/{len(val_scenarios)}] ERROR: {e}\")\n",
    "\n",
    "if val_rewards:\n",
    "    print(f\"\\nValidation results:\")\n",
    "    print(f\"  Mean reward: {sum(val_rewards)/len(val_rewards):.4f}\")\n",
    "    print(f\"  Max reward: {max(val_rewards):.4f}\")\n",
    "    print(f\"  Mean bridges: {sum(val_bridges)/len(val_bridges):.1f}\")\n",
    "    print(f\"  Nonzero reward: {sum(1 for r in val_rewards if r > 0)/len(val_rewards):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_scenario = val_scenarios[0]\n",
    "print(f\"Question: {test_scenario['question']}\")\n",
    "print(f\"Expected: {test_scenario['answer']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = run_episode(test_scenario, model, tokenizer)\n",
    "\n",
    "print(f\"\\nReward: {result['reward']:.4f}\")\n",
    "print(f\"Bridges: {result['num_bridges']}\")\n",
    "print(f\"Turns: {result['num_turns']}\")\n",
    "\n",
    "# Show full trajectory\n",
    "print(f\"\\n--- Trajectory ({len(result['messages'])} messages) ---\")\n",
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    role = msg[\"role\"]\n",
    "    content = msg[\"content\"]\n",
    "    if role == \"system\":\n",
    "        print(f\"\\n[SYSTEM]: ({len(content)} chars)\")\n",
    "    elif role == \"user\" and \"<tool_response>\" in content:\n",
    "        display = content[:300] + \"...\" if len(content) > 300 else content\n",
    "        print(f\"\\n[TOOL_RESPONSE]: {display}\")\n",
    "    elif role == \"user\":\n",
    "        print(f\"\\n[USER]: {content}\")\n",
    "    elif role == \"assistant\":\n",
    "        display = content[:400] + \"...\" if len(content) > 400 else content\n",
    "        print(f\"\\n[ASSISTANT]: {display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "\n",
    "Save as LoRA adapter or merged weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(str(output_path / \"final\"))\n",
    "tokenizer.save_pretrained(str(output_path / \"final\"))\n",
    "print(f\"LoRA adapter saved to: {output_path / 'final'}\")\n",
    "\n",
    "# Optionally merge and save as full model (16-bit)\n",
    "# model.save_pretrained_merged(str(output_path / \"merged_16bit\"), tokenizer, save_method=\"merged_16bit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
