{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSW Sleep-Time Agent — Multi-Turn GRPO Training with ART\n",
    "\n",
    "Train a Qwen3-30B-A3B (MoE) model to explore GSW structures and create bridge QA pairs\n",
    "using [OpenPipe ART](https://github.com/openpipe/ART) for multi-turn reinforcement learning.\n",
    "\n",
    "**Architecture:**\n",
    "- ART handles inference (vLLM), LoRA training (Unsloth/torchtune), and GRPO updates\n",
    "- GSW tools execute locally against `GSWEnvironment.step()`\n",
    "- Reward: bridge-F1 against MuSiQue gold answers (verifiable, no LLM judge needed)\n",
    "\n",
    "**Backends:**\n",
    "- `ServerlessBackend`: Qwen3-30B-A3B on ART cloud (requires WANDB_API_KEY)\n",
    "- `LocalBackend`: Qwen3-30B-A3B on local GPUs (4x A6000 48GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites / Troubleshooting\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. **Fresh kernel**: If you encounter CUDA errors, restart the kernel completely\n",
    "   (Kernel > Restart) before re-running.\n",
    "\n",
    "2. **flash-attn version**: Must be >= 2.8.3. Check with:\n",
    "   ```python\n",
    "   import flash_attn; print(flash_attn.__version__)\n",
    "   ```\n",
    "   If outdated: `pip install flash-attn>=2.8.3 --no-build-isolation`\n",
    "\n",
    "3. **Stale compiled cache**: Delete `./unsloth_compiled_cache/` if you upgraded\n",
    "   PyTorch, CUDA, or unsloth since the last run.\n",
    "\n",
    "4. **GPU memory**: Qwen3-30B-A3B requires ~20GB+ VRAM. Ensure no other processes\n",
    "   hold GPU memory. Check with `nvidia-smi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# ---- Model download directory ----\nos.environ[\"HF_HOME\"] = \"/mnt/SSD3/yigit\"\n\n# ---- Backend Selection ----\nUSE_SERVERLESS = False   # ART cloud (needs WANDB_API_KEY)\nUSE_LOCAL = True         # Local GPUs (4x A6000 48GB)\n\n# ---- Model ----\nBASE_MODEL = \"Qwen/Qwen3-30B-A3B\"  # MoE: 30B total, ~3B active\n\n# ---- Training ----\nMAX_TURNS = 30           # Max tool calls per episode\nROLLOUTS_PER_GROUP = 4   # GRPO group size\nGROUPS_PER_STEP = 2      # Scenarios per training step\nNUM_EPOCHS = 5\nLEARNING_RATE = 1e-5\nMAX_STEPS = 200          # Stop after this many steps (None for full run)\nVALIDATION_INTERVAL = 10 # Run validation every N steps\n\n# ---- Data ----\nINDEX_PATH = \"/home/yigit/codebase/gsw-memory/data/rl_training/index.json\"\nVAL_SPLIT = 0.05\n\n# ---- API Keys ----\n# os.environ[\"WANDB_API_KEY\"] = \"\"  # Required for both backends\n\nprint(f\"Backend: {'Serverless' if USE_SERVERLESS else 'Local'}\")\nprint(f\"Model: {BASE_MODEL}\")\nprint(f\"GRPO group size: {ROLLOUTS_PER_GROUP}\")\nprint(f\"HF_HOME: {os.environ['HF_HOME']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(INDEX_PATH) as f:\n",
    "    all_scenarios = json.load(f)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(all_scenarios)\n",
    "\n",
    "n_val = max(1, int(len(all_scenarios) * VAL_SPLIT))\n",
    "val_scenarios = all_scenarios[:n_val]\n",
    "train_scenarios = all_scenarios[n_val:]\n",
    "\n",
    "print(f\"Total scenarios: {len(all_scenarios)}\")\n",
    "print(f\"Train: {len(train_scenarios)}, Val: {len(val_scenarios)}\")\n",
    "print(f\"\\nSample scenario:\")\n",
    "print(f\"  Question: {train_scenarios[0]['question']}\")\n",
    "print(f\"  Answer: {train_scenarios[0]['answer']}\")\n",
    "print(f\"  GSW dirs: {train_scenarios[0]['gsw_dirs']}\")\n",
    "print(f\"  Num hops: {train_scenarios[0].get('num_hops', '?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model + Backend Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# ---- 1. Clear any stale CUDA state ----\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"CUDA caches cleared and synchronized.\")\n",
    "else:\n",
    "    print(\"WARNING: No CUDA GPUs detected!\")\n",
    "\n",
    "# ---- 2. Report GPU status ----\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    free, _ = torch.cuda.mem_get_info(i)\n",
    "    free_gb = free / 1024**3\n",
    "    print(f\"  GPU {i}: {name} | {total:.1f} GB total | {free_gb:.1f} GB free\")\n",
    "\n",
    "# ---- 3. Validate flash-attn version ----\n",
    "try:\n",
    "    import flash_attn\n",
    "    fa_version = flash_attn.__version__\n",
    "    print(f\"\\nflash-attn version: {fa_version}\")\n",
    "    from packaging.version import Version\n",
    "    if Version(fa_version) < Version(\"2.8.3\"):\n",
    "        print(f\"  WARNING: flash-attn {fa_version} < 2.8.3 (required)\")\n",
    "        print(f\"  This may cause CUDA illegal memory access errors.\")\n",
    "        print(f\"  Fix: pip install flash-attn>=2.8.3 --no-build-isolation\")\n",
    "except ImportError:\n",
    "    print(\"\\nflash-attn: not installed\")\n",
    "\n",
    "# ---- 4. Check for stale unsloth compiled cache ----\n",
    "from pathlib import Path\n",
    "cache_dir = Path(\"./unsloth_compiled_cache\")\n",
    "if cache_dir.exists():\n",
    "    num_files = sum(1 for _ in cache_dir.rglob(\"*\") if _.is_file())\n",
    "    print(f\"\\nunsloth_compiled_cache: {num_files} files found\")\n",
    "    print(\"  If you encounter CUDA errors, try deleting this directory:\")\n",
    "    print(f\"  rm -rf {cache_dir.resolve()}\")\n",
    "else:\n",
    "    print(\"\\nunsloth_compiled_cache: not found (clean state)\")\n",
    "\n",
    "# ---- 5. Set vLLM memory limit ----\n",
    "os.environ.setdefault(\"VLLM_GPU_MEMORY_UTILIZATION\", \"0.85\")\n",
    "\n",
    "# Uncomment for synchronous CUDA errors (slower but pinpoints location):\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "print(\"\\nGPU diagnostics complete. Proceed to model registration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import art\nfrom art import dev\nimport shutil\n\n# Clear stale ART state (needed when changing _internal_config)\nart_dir = Path(\"./.art/gsw-sleep-time\")\nif art_dir.exists():\n    shutil.rmtree(art_dir)\n    print(\"Cleared stale .art state\")\n\n# Qwen3-30B-A3B is MoE — Unsloth doesn't support fast_inference for MoE.\n# Use _decouple_vllm_and_unsloth=True to run Unsloth and vLLM separately.\nart_model = art.TrainableModel(\n    name=\"gsw-sleep-agent-001\",\n    project=\"gsw-sleep-time\",\n    base_model=BASE_MODEL,\n    _internal_config=dev.InternalModelConfig(\n        _decouple_vllm_and_unsloth=True,\n        init_args={\n            \"load_in_4bit\": False,\n            \"max_seq_length\": 8192,\n        },\n        engine_args={\n            \"tensor_parallel_size\": 2,\n            \"enforce_eager\": True,\n            \"max_model_len\": 8192,\n            \"dtype\": \"bfloat16\",\n            \"gpu_memory_utilization\": 0.75,\n            \"enable_sleep_mode\": True,\n        },\n    ),\n)\n\nif USE_SERVERLESS:\n    from art.serverless.backend import ServerlessBackend\n    backend = ServerlessBackend()\n    print(\"Using ServerlessBackend (ART cloud)\")\nelse:\n    from art.local import LocalBackend\n    backend = LocalBackend(path=\"./.art\")\n    print(\"Using LocalBackend (local GPUs)\")\n\ntry:\n    await art_model.register(backend)\nexcept RuntimeError as e:\n    if \"CUDA\" in str(e) or \"illegal memory access\" in str(e):\n        print(f\"\\nCUDA error during registration: {e}\")\n        print(\"\\nAttempting recovery:\")\n        print(\"  1. Clearing unsloth compiled cache...\")\n        cache_dir = Path(\"./unsloth_compiled_cache\")\n        if cache_dir.exists():\n            shutil.rmtree(cache_dir)\n            print(\"     Deleted unsloth_compiled_cache/\")\n        print(\"  2. Clearing CUDA state...\")\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(\"\\n  RECOMMENDED: Restart the Jupyter kernel and re-run all cells.\")\n        raise\n    else:\n        raise\n\nprint(f\"Model registered. Current step: {await art_model.get_step()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSW Tool Definitions\n",
    "\n",
    "Define the 10 GSW tools in OpenAI function-calling format.\n",
    "These schemas are passed to the model via `tools=` so it knows what tools are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSW_TOOL_SCHEMAS = [\n",
    "    # ---- Discovery ----\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_entity_documents\",\n",
    "            \"description\": \"Get list of document IDs that mention this entity.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_name\": {\"type\": \"string\", \"description\": \"Entity to search for\"}\n",
    "                },\n",
    "                \"required\": [\"entity_name\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_document_entities\",\n",
    "            \"description\": \"Get list of entities mentioned in this document.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"doc_id\": {\"type\": \"string\", \"description\": \"Document ID (e.g., 'doc_3')\"}\n",
    "                },\n",
    "                \"required\": [\"doc_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # ---- Context ----\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_entity_context\",\n",
    "            \"description\": \"Get all QA pairs, roles, states, and relationships for an entity. Pass doc_id for single doc, or omit for merged context from all docs.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_name\": {\"type\": \"string\", \"description\": \"Entity to get context for\"},\n",
    "                    \"doc_id\": {\"type\": \"string\", \"description\": \"Optional document ID (e.g., 'doc_4'). Omit for merged context.\"}\n",
    "                },\n",
    "                \"required\": [\"entity_name\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"reconcile_entity_across_docs\",\n",
    "            \"description\": \"Merge all information about an entity from all documents into unified view. Use this to see complete picture of an entity.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_name\": {\"type\": \"string\", \"description\": \"Entity to reconcile\"}\n",
    "                },\n",
    "                \"required\": [\"entity_name\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # ---- Bridges ----\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"create_bridge_qa\",\n",
    "            \"description\": \"Create a bridge QA pair connecting information across documents.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\"type\": \"string\", \"description\": \"Bridge question\"},\n",
    "                    \"answer\": {\"type\": \"string\", \"description\": \"Bridge answer\"},\n",
    "                    \"source_docs\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"List of source document IDs\"\n",
    "                    },\n",
    "                    \"reasoning\": {\"type\": \"string\", \"description\": \"How this bridge was derived\"},\n",
    "                    \"confidence\": {\"type\": \"number\", \"description\": \"Confidence score (0-1, default 0.9)\"},\n",
    "                    \"entities_involved\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Entities mentioned in bridge\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\", \"answer\", \"source_docs\", \"reasoning\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_bridge_statistics\",\n",
    "            \"description\": \"Get statistics on bridges created so far (total, coverage, quality).\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # ---- Strategy ----\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"mark_entity_explored\",\n",
    "            \"description\": \"Mark an entity as explored. Call this when done exploring an entity.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_name\": {\"type\": \"string\", \"description\": \"Entity that was explored\"},\n",
    "                    \"num_bridges_created\": {\"type\": \"integer\", \"description\": \"Number of bridges created for this entity\"}\n",
    "                },\n",
    "                \"required\": [\"entity_name\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # ---- Exploration Tracking ----\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"plan_entity_exploration\",\n",
    "            \"description\": \"Create exploration plan for entity showing all relationships to check. Call ONCE after reconcile_entity_across_docs.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_name\": {\"type\": \"string\", \"description\": \"Entity being explored\"},\n",
    "                    \"relationships\": {\"type\": \"object\", \"description\": \"merged_relationships dict from reconcile_entity_across_docs output\"}\n",
    "                },\n",
    "                \"required\": [\"entity_name\", \"relationships\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"mark_relationship_explored\",\n",
    "            \"description\": \"Mark a relationship as explored after checking all its documents. Returns updated checklist.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_name\": {\"type\": \"string\", \"description\": \"Main entity being explored\"},\n",
    "                    \"relationship_name\": {\"type\": \"string\", \"description\": \"Name of the related entity just explored\"},\n",
    "                    \"bridges_created\": {\"type\": \"integer\", \"description\": \"Number of bridges created for this relationship\"}\n",
    "                },\n",
    "                \"required\": [\"entity_name\", \"relationship_name\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_exploration_status\",\n",
    "            \"description\": \"Check which relationships explored vs pending. Use before mark_entity_explored to verify completeness.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_name\": {\"type\": \"string\", \"description\": \"Entity to check status for\"}\n",
    "                },\n",
    "                \"required\": [\"entity_name\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(GSW_TOOL_SCHEMAS)} tool schemas:\")\n",
    "for t in GSW_TOOL_SCHEMAS:\n",
    "    print(f\"  - {t['function']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout Function\n",
    "\n",
    "Runs one episode: the agent explores GSW structures via tool calls,\n",
    "creating bridge QA pairs. The trajectory captures all messages + tool calls.\n",
    "Reward is computed from bridge quality (F1 against gold answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import art\n",
    "\n",
    "from gsw_memory.sleep_time.environment import GSWEnvironment\n",
    "from gsw_memory.sleep_time.reward import compute_reward\n",
    "from gsw_memory.sleep_time.prompts import SLEEP_TIME_SYSTEM_PROMPT\n",
    "from gsw_memory.sleep_time.entity_search import EntitySearcher\n",
    "\n",
    "\n",
    "async def rollout(model: art.Model, scenario: dict, step: int = 0) -> art.Trajectory:\n",
    "    \"\"\"\n",
    "    Run one multi-turn exploration episode.\n",
    "\n",
    "    The agent calls GSW tools to explore entity relationships and create\n",
    "    bridge QA pairs. Tools execute locally against GSWEnvironment.\n",
    "    \"\"\"\n",
    "    # Build GSW environment for this episode\n",
    "    gsw_dirs = scenario[\"gsw_dirs\"]\n",
    "    gsw_root = str(Path(gsw_dirs[0]).parent)\n",
    "    searcher = EntitySearcher(path_to_gsw_files=gsw_root, verbose=False)\n",
    "    env = GSWEnvironment(\n",
    "        entity_searcher=searcher,\n",
    "        question=scenario[\"question\"],\n",
    "        gold_answer=scenario[\"answer\"],\n",
    "        gold_decomposition=scenario.get(\"decomposition\", []),\n",
    "        max_turns=MAX_TURNS,\n",
    "    )\n",
    "    env.reset()\n",
    "\n",
    "    user_content = (\n",
    "        f\"Question: {scenario['question']}\\n\\n\"\n",
    "        f\"Explore the GSW corpus to find multi-hop bridge connections \"\n",
    "        f\"that help answer this question. Use the available tools systematically.\"\n",
    "    )\n",
    "\n",
    "    traj = art.Trajectory(\n",
    "        reward=0.0,\n",
    "        messages_and_choices=[\n",
    "            {\"role\": \"system\", \"content\": SLEEP_TIME_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ],\n",
    "        tools=GSW_TOOL_SCHEMAS,\n",
    "        metadata={\n",
    "            \"scenario_id\": scenario.get(\"id\", \"\"),\n",
    "            \"step\": step,\n",
    "            \"question\": scenario[\"question\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    client = AsyncOpenAI(\n",
    "        base_url=model.inference_base_url,\n",
    "        api_key=model.inference_api_key,\n",
    "    )\n",
    "\n",
    "    for turn in range(MAX_TURNS):\n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                model=model.get_inference_name(),\n",
    "                temperature=0.7,\n",
    "                messages=traj.messages(),\n",
    "                tools=traj.tools,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            traj.log(f\"Inference error at turn {turn}: {e}\")\n",
    "            break\n",
    "\n",
    "        response_message = response.choices[0].message\n",
    "        traj.messages_and_choices.append(response.choices[0])\n",
    "\n",
    "        # No tool calls = agent stopped (freeform response)\n",
    "        if not response_message.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute each tool call against the GSW environment\n",
    "        try:\n",
    "            for tool_call in response_message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                # Execute against live GSW environment\n",
    "                obs, done = env.step(tool_name, tool_args)\n",
    "\n",
    "                traj.messages_and_choices.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": obs,\n",
    "                })\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            traj.log(f\"Tool execution error at turn {turn}: {e}\")\n",
    "            traj.messages_and_choices.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"name\": tool_name,\n",
    "                \"content\": json.dumps({\"error\": str(e)}),\n",
    "            })\n",
    "\n",
    "        if env.done:\n",
    "            break\n",
    "\n",
    "    # Compute reward from bridges created during the episode\n",
    "    bridges = env.get_bridges()\n",
    "    traj.reward = compute_reward(\n",
    "        bridges=bridges,\n",
    "        gold_answer=scenario[\"answer\"],\n",
    "        gold_decomposition=scenario.get(\"decomposition\", []),\n",
    "        gold_aliases=scenario.get(\"answer_aliases\", []),\n",
    "    )\n",
    "    traj.metrics[\"num_bridges\"] = len(bridges)\n",
    "    traj.metrics[\"num_turns\"] = env.turn\n",
    "    traj.metrics[\"env_done\"] = int(env.done)\n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "print(\"Rollout function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rollout\n",
    "\n",
    "Run a single rollout to verify the pipeline works before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scenario = train_scenarios[0]\n",
    "print(f\"Test scenario: {test_scenario['question']}\")\n",
    "print(f\"Expected answer: {test_scenario['answer']}\")\n",
    "print(f\"GSW dirs: {test_scenario['gsw_dirs']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_traj = await rollout(art_model, test_scenario, step=0)\n",
    "\n",
    "print(f\"\\nTrajectory summary:\")\n",
    "print(f\"  Reward: {test_traj.reward:.4f}\")\n",
    "print(f\"  Bridges created: {test_traj.metrics.get('num_bridges', 0)}\")\n",
    "print(f\"  Turns used: {test_traj.metrics.get('num_turns', 0)}\")\n",
    "print(f\"  Episode done: {test_traj.metrics.get('env_done', 0)}\")\n",
    "\n",
    "# Show tool calls\n",
    "messages = test_traj.messages()\n",
    "print(f\"\\nMessages ({len(messages)} total):\")\n",
    "for i, msg in enumerate(messages):\n",
    "    role = msg.get(\"role\", \"unknown\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    tool_calls = msg.get(\"tool_calls\", [])\n",
    "    if role == \"system\":\n",
    "        print(f\"  [{i}] SYSTEM: (system prompt, {len(content)} chars)\")\n",
    "    elif role == \"user\":\n",
    "        print(f\"  [{i}] USER: {content[:100]}...\")\n",
    "    elif role == \"assistant\":\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                print(f\"  [{i}] ASSISTANT tool_call: {tc['function']['name']}({tc['function']['arguments'][:80]})\")\n",
    "        if content:\n",
    "            print(f\"  [{i}] ASSISTANT: {content[:100]}...\")\n",
    "    elif role == \"tool\":\n",
    "        name = msg.get(\"name\", \"?\")\n",
    "        print(f\"  [{i}] TOOL ({name}): {content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.utils import iterate_dataset\n",
    "\n",
    "training_iterator = iterate_dataset(\n",
    "    train_scenarios,\n",
    "    groups_per_step=GROUPS_PER_STEP,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    initial_step=await art_model.get_step(),\n",
    ")\n",
    "\n",
    "for batch in training_iterator:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Step {batch.step} | Epoch {batch.epoch} | Epoch step {batch.epoch_step}\")\n",
    "    print(f\"Batch: {len(batch.items)} scenarios x {ROLLOUTS_PER_GROUP} rollouts\")\n",
    "\n",
    "    # ---- Generate rollouts ----\n",
    "    train_groups = []\n",
    "    for scenario in batch.items:\n",
    "        train_groups.append(\n",
    "            art.TrajectoryGroup(\n",
    "                rollout(art_model, scenario, step=batch.step)\n",
    "                for _ in range(ROLLOUTS_PER_GROUP)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    finished_groups = await art.gather_trajectory_groups(\n",
    "        train_groups,\n",
    "        pbar_desc=f\"step {batch.step}\",\n",
    "        max_exceptions=ROLLOUTS_PER_GROUP * len(batch.items),\n",
    "    )\n",
    "\n",
    "    # ---- Log metrics ----\n",
    "    all_rewards = []\n",
    "    all_bridges = []\n",
    "    all_turns = []\n",
    "    for group in finished_groups:\n",
    "        for traj in group.trajectories:\n",
    "            all_rewards.append(traj.reward)\n",
    "            all_bridges.append(traj.metrics.get(\"num_bridges\", 0))\n",
    "            all_turns.append(traj.metrics.get(\"num_turns\", 0))\n",
    "\n",
    "    if all_rewards:\n",
    "        print(f\"  Rewards: mean={sum(all_rewards)/len(all_rewards):.3f}, \"\n",
    "              f\"max={max(all_rewards):.3f}, min={min(all_rewards):.3f}\")\n",
    "        print(f\"  Bridges: mean={sum(all_bridges)/len(all_bridges):.1f}, \"\n",
    "              f\"max={max(all_bridges)}\")\n",
    "        print(f\"  Turns: mean={sum(all_turns)/len(all_turns):.1f}\")\n",
    "\n",
    "    # ---- Validation ----\n",
    "    if batch.step % VALIDATION_INTERVAL == 0:\n",
    "        print(f\"  Running validation ({len(val_scenarios)} scenarios)...\")\n",
    "        val_groups = []\n",
    "        for scenario in val_scenarios:\n",
    "            val_groups.append(\n",
    "                art.TrajectoryGroup(\n",
    "                    [rollout(art_model, scenario, step=batch.step)]\n",
    "                )\n",
    "            )\n",
    "        finished_val = await art.gather_trajectory_groups(\n",
    "            val_groups,\n",
    "            pbar_desc=\"val\",\n",
    "            max_exceptions=len(val_scenarios),\n",
    "        )\n",
    "        val_rewards = [\n",
    "            t.reward\n",
    "            for g in finished_val\n",
    "            for t in g.trajectories\n",
    "        ]\n",
    "        if val_rewards:\n",
    "            print(f\"  Val rewards: mean={sum(val_rewards)/len(val_rewards):.3f}, \"\n",
    "                  f\"max={max(val_rewards):.3f}\")\n",
    "        await art_model.log(finished_val, split=\"val\")\n",
    "\n",
    "    # ---- Train (GRPO update) ----\n",
    "    await art_model.delete_checkpoints()\n",
    "    await art_model.train(\n",
    "        finished_groups,\n",
    "        config=art.TrainConfig(learning_rate=LEARNING_RATE),\n",
    "    )\n",
    "    print(f\"  Training step {batch.step} complete.\")\n",
    "\n",
    "    if MAX_STEPS and batch.step >= MAX_STEPS:\n",
    "        print(f\"\\nReached max_steps={MAX_STEPS}. Stopping.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing the trained model...\\n\")\n",
    "\n",
    "test_scenario = val_scenarios[0]\n",
    "print(f\"Question: {test_scenario['question']}\")\n",
    "print(f\"Expected answer: {test_scenario['answer']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result_traj = await rollout(art_model, test_scenario, step=0)\n",
    "\n",
    "print(f\"\\nReward: {result_traj.reward:.4f}\")\n",
    "print(f\"Bridges: {result_traj.metrics.get('num_bridges', 0)}\")\n",
    "print(f\"Turns: {result_traj.metrics.get('num_turns', 0)}\")\n",
    "\n",
    "# Show full trajectory\n",
    "messages = result_traj.messages()\n",
    "print(f\"\\n--- Full Trajectory ({len(messages)} messages) ---\")\n",
    "for i, msg in enumerate(messages):\n",
    "    role = msg.get(\"role\", \"unknown\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    tool_calls = msg.get(\"tool_calls\", [])\n",
    "\n",
    "    if role == \"system\":\n",
    "        print(f\"\\n[SYSTEM]: (prompt, {len(content)} chars)\")\n",
    "    elif role == \"user\":\n",
    "        print(f\"\\n[USER]: {content}\")\n",
    "    elif role == \"assistant\":\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                print(f\"\\n[ASSISTANT → {tc['function']['name']}]: {tc['function']['arguments'][:200]}\")\n",
    "        if content:\n",
    "            print(f\"\\n[ASSISTANT]: {content[:300]}\")\n",
    "    elif role == \"tool\":\n",
    "        name = msg.get(\"name\", \"?\")\n",
    "        display = content[:300] + \"...\" if len(content) > 300 else content\n",
    "        print(f\"\\n[TOOL {name}]: {display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "\n",
    "ART saves checkpoints automatically. For explicit export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model checkpoints are saved by ART in the backend's path (.art/ for local)\n",
    "# For ServerlessBackend, checkpoints are W&B Artifacts\n",
    "\n",
    "current_step = await art_model.get_step()\n",
    "print(f\"Final model step: {current_step}\")\n",
    "print(f\"Checkpoints saved at: .art/ (LocalBackend) or W&B Artifacts (ServerlessBackend)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}